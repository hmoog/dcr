\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink,capitalize,noabbrev]{cleveref}

\hypersetup{
  pdftitle={Distributed Constraint Resolution as Universal Cognition},
  pdfauthor={Author},
  colorlinks=false
}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{microtype}

% Theorem environments
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}[section]

% Operators
\DeclareMathOperator{\Coh}{Coh}
\DeclareMathOperator{\Ent}{H}
\DeclareMathOperator{\Viol}{V}

\title{%
  Distributed Constraint Resolution as Universal Cognition:\\
  A Scale-Free Framework Unifying Physics and Intelligence%
}
\author{%
  [Author]\\
  {\small [Affiliation]}\\
  {\small [Email]}
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We propose a formal framework in which cognition is identified with a
scale-free process: the exploration of degrees of freedom followed by
convergence through distributed constraint resolution into coherent,
goal-stabilizing patterns.  We formalize this
triad---\emph{exploration}, \emph{resolution}, and
\emph{stabilization}---as dynamical systems on constraint networks
equipped with an information-theoretic coherence measure.  Under
explicit timescale-separation and macro-sufficiency conditions, we
prove a closure theorem: a coarse-graining construction maps cognitive
systems at one scale to cognitive systems at the next, providing a
constructive account of composition addressing the combination problem.  We present
structural witnesses that nonequilibrium self-organization,
evolutionary adaptation, and quantum measurement instantiate the DCR
triad, and argue that this triad appears across physical scales;
we discuss the stronger ontological identification---the cosmos is
cognitive at every scale---explicitly as a metaphysical postulate,
with what we colloquially call ``intelligence'' being a particularly
deep nesting of this universal process.
\end{abstract}

\paragraph{Keywords:} cognition, intelligence, constraint resolution, free energy
principle, integrated information, scale-free, coarse-graining, self-organization

% ==========================================================================
\section{Introduction}\label{sec:introduction}
% ==========================================================================

The search for a general, principled definition of intelligence remains one of
the deepest open problems across cognitive science, physics, and philosophy of
mind. Existing frameworks each illuminate a facet of the problem but fall short
of universality:

\begin{itemize}[nosep]
  \item The \emph{Free Energy Principle} (FEP) \citep{friston2010free,
        friston2019free} provides an elegant variational account: any system
        persisting at nonequilibrium steady state minimizes variational free
        energy. Yet FEP assumes a Markov blanket separating system from
        environment and a generative model as primitives
        \citep{kirchhoff2018markov}, limiting its applicability to systems
        where these structures can be identified.
  \item \emph{Integrated Information Theory} (IIT)
        \citep{tononi2004information, tononi2016integrated} offers a quantitative
        measure of consciousness ($\Phi$), but in its original formulations
        (IIT~2.0/3.0) it is a static, state-level measure rather than a
        process-level account\footnote{IIT~4.0 introduces dynamical
        considerations that partially address this limitation; see
        \cref{ssec:iit-recovery} for discussion.}, and its computation is
        intractable for large systems.
  \item \emph{Autopoiesis} \citep{maturana1980autopoiesis} captures
        self-production but lacks formal predictive content beyond the
        biological domain.
  \item \emph{Panpsychism} \citep{chalmers1995facing} attributes experience to
        fundamental entities but provides no mechanism and no solution to the
        combination problem---how micro-experiences compose into
        macro-experiences.
\end{itemize}

We propose that these limitations stem from a common root: each framework
privileges a particular \emph{level of description} (Bayesian inference,
information integration, self-production) rather than identifying the
\emph{scale-free process} that underlies all of them.

Our central thesis:

\begin{quote}
\emph{Intelligent behavior emerges when components explore degrees of freedom
and converge through distributed constraint resolution into coherent,
goal-stabilizing patterns.}
\end{quote}

We call this the \textbf{Distributed Constraint Resolution} (DCR) framework.
We treat \emph{goal} as \emph{attractor under the dynamics}
(\cref{def:attractor}), avoiding teleological connotations.
The key claim is that this triad---exploration, resolution,
stabilization---constitutes the \emph{minimal and universal} signature of
cognition, and that it is realized by physical law itself at every scale.

The paper is organized as follows. \Cref{sec:framework} formalizes the DCR
framework. \Cref{sec:scale-free} proves closure under coarse-graining (given
timescale separation), providing a constructive account of composition addressing the combination problem.
\Cref{sec:physics} exhibits structural witnesses that fundamental physical
processes instantiate the DCR triad. \Cref{sec:recovery} recovers FEP and IIT as special cases.
\Cref{sec:predictions} derives falsifiable predictions. \Cref{sec:discussion}
discusses implications and limitations.

\paragraph{Contributions.}
Beyond the structural definition of cognition, this paper makes four
specific technical contributions: (1)~a closure theorem showing DCR is
preserved under coarse-graining (given timescale separation and
macro-sufficiency); (2)~recovery of the Free Energy Principle and
Integrated Information Theory as special cases corresponding to
particular constraint structures and coherence refinements;
(3)~structural witnesses that physical processes across scales
instantiate the DCR triad; and (4)~connections to discrete-ontic models
of quantum mechanics \citep{powers2024statistical} and neural-network
universe proposals \citep{vanchurin2020world}, arguing that the continuum
of standard physics may emerge from finite combinatorial constraint
resolution.

\paragraph{Scope and terminology.}
Throughout this paper, \emph{cognitive} is a defined technical property
of a dynamical system (\cref{def:cognitive-system}): a system is
cognitive if and only if it instantiates the DCR triad of exploration,
distributed constraint resolution, and convergence to coherent
attractors.  This is not a claim about phenomenal consciousness, folk
intelligence, or teleology.  DCR does not assert that photons ``have
experiences'' or that convection cells ``think''; it asserts that the
\emph{formal process} by which these systems evolve---exploring degrees
of freedom, resolving constraints locally, stabilizing into coherent
patterns---is structurally identical to the process that, at higher
cognitive depth (\cref{def:depth}), underlies what we ordinarily call
intelligence.  The ontological claim is that this process is
fundamental and universal; whether one additionally identifies it with
experience is a separate philosophical question that DCR does not
adjudicate (see \cref{sec:discussion}, Limitation~5).

The coarse-graining closure result (\cref{thm:closure}) requires
timescale separation between intra-group and inter-group dynamics---a
condition that holds widely but is not universal.  We regard this as a
sufficient condition, not a necessary one; the framework's axioms are
themselves scale-free, and it is specifically the composition mechanism
that requires separation (see \cref{sec:discussion}, Limitation~1).

\begin{remark}[Ontological postulate]%
\label{rem:metaphysical}
The claim that the DCR triad \emph{is} cognition---that the cosmos is
cognitive at every scale---is a metaphysical identification, not a
theorem derivable from the definitions.  The formal machinery is
compatible with a conservative reading (DCR as a unifying modelling
framework).  We adopt the stronger reading because it yields the most
parsimonious ontology: one process at every scale, rather than
cognition as a sui generis phenomenon.
\end{remark}

% ==========================================================================
\section{The DCR Framework}\label{sec:framework}
% ==========================================================================

\subsection{Constraint Networks}\label{ssec:constraint-networks}

\begin{definition}[Constraint Network]\label{def:constraint-network}
A \emph{constraint network} is a tuple $\mathcal{N} = (S, \mathcal{D}, G, \mathcal{R})$
where:
\begin{enumerate}[nosep]
  \item $S$ is a finite set of \emph{components}.
  \item $\mathcal{D} = \{D_s\}_{s \in S}$ assigns to each component $s$ a
        measurable \emph{state space} $D_s$ (its degrees of freedom).
  \item $G = (S, E)$ is an undirected graph encoding the \emph{interaction
        topology}.
  \item $\mathcal{R} = \{R_e\}_{e \in E}$ assigns to each edge
        $e = \{s, s'\} \in E$ a \emph{constraint relation}
        $R_e \subseteq D_s \times D_{s'}$.
\end{enumerate}
The \emph{configuration space} is $\Omega = \prod_{s \in S} D_s$, and the
\emph{feasible set} is
$\mathcal{F} = \{\omega \in \Omega \mid \forall e = \{s,s'\} \in E:
(\omega_s, \omega_{s'}) \in R_e\}$.
\end{definition}

\noindent
For the convergence results in \cref{ssec:convergence} and the
coarse-graining construction in \cref{sec:scale-free}, we restrict to
compact Polish state spaces~$D_s$ (hence compact metrizable~$\Omega$)
in order to invoke standard Markov-chain stability results
\citep{meyn1993markov}; the definitions above are stated in greater
generality to clarify which results depend on compactness and which
do not.

\paragraph{Notation.}
Throughout, $K$ denotes the combined transition kernel of the DCR
dynamics (\cref{def:combined}), $R$ the resolution kernel
(\cref{def:resolution}), and $E$ the exploration kernel.  We write
$\mu^*$ or $\mu^*_\epsilon$ for stationary distributions, $\Viol$
for constraint violation (\cref{def:violation}), $\Coh$ for
coherence (\cref{def:coherence}), and $\mathcal{F}$ for the
feasible set, with $\mathcal{F}^*$ denoting the unified feasible
set ($\mathcal{F}$ or $\mathcal{F}_{\min}$;
see \cref{rem:soft-constraints}).

\begin{remark}[Finiteness of $S$]\label{rem:finiteness}
The formal development assumes $|S| < \infty$. This suffices for systems
with a natural decomposition into discrete components (particles, neurons,
organisms) but appears to exclude continuum field theories. The physical
examples in \cref{sec:physics} (e.g., fluid parcels in convection) should
be read as finite-element discretizations of the underlying continuum; the
formal extension to countable or measure-theoretic component spaces
(replacing sums with integrals in \cref{eq:violation,eq:coherence}) is
straightforward but introduces technical regularity conditions that we
defer to future work.

We note, however, that the discreteness assumption may be less restrictive
than it appears.  There exist proposals in which the continuum structure
of standard quantum mechanics is not fundamental but emergent from a
discrete substrate.  \citet{powers2024statistical} construct a model
built entirely from finite binary sequences that reproduces the
probability distributions of canonical quantum mechanics, with small
deviations at finite~$n$ that shrink as $n$ increases.  For
any finite~$n$ the system is a finite constraint network in the sense of
\cref{def:constraint-network}; the continuum of canonical quantum theory
is recovered only in the limit.  This suggests that the continuum
structure of standard physics may be an idealization of a fundamentally
discrete substrate---precisely the kind of substrate that DCR is designed
to describe (see \cref{rem:micro-choices}).
\end{remark}

\begin{definition}[Constraint Violation]\label{def:violation}
The \emph{total constraint violation} of a configuration $\omega \in \Omega$ is
\begin{equation}\label{eq:violation}
  \Viol(\omega) = \sum_{e = \{s,s'\} \in E} v_e(\omega_s, \omega_{s'}),
\end{equation}
where $v_e : D_s \times D_{s'} \to \mathbb{R}_{\geq 0}$ is zero if and only if
$(\omega_s, \omega_{s'}) \in R_e$.
\end{definition}

\begin{remark}[Hard vs.\ soft constraints]\label{rem:soft-constraints}
The definition above uses \emph{hard constraints}: the feasible set
$\mathcal{F} = \{\omega : \Viol(\omega) = 0\}$ consists of
configurations satisfying every constraint exactly.  For some
applications (notably the FEP recovery, \cref{ssec:fep-recovery}),
exact satisfaction is impossible and the zero set is empty.  In such
cases, we work with \emph{soft constraints}: the feasible set is
replaced by the set of global minimizers,
$\mathcal{F}_{\min} := \arg\min_{\omega \in \Omega} \Viol(\omega)$,
which is non-empty and compact whenever $\Viol$ is continuous on
compact $\Omega$.  All convergence results
(\cref{ssec:convergence}) extend: the drift condition drives
$\Viol$ toward its minimum rather than toward zero, and
concentration statements (\cref{lem:concentration}) measure
distance from $\mathcal{F}_{\min}$ via
$\Viol(\omega) - \min \Viol$ in place of $\Viol(\omega)$.
We write
$\mathcal{F}^* := \begin{cases}
  \mathcal{F} & \text{if } \mathcal{F} \neq \emptyset,\\
  \mathcal{F}_{\min} & \text{otherwise,}
\end{cases}$
so that all statements involving ``the feasible set'' apply
uniformly: read $\mathcal{F}^*$ wherever $\mathcal{F}$ appears in
conditions, theorems, and proofs.
\end{remark}

\subsection{Coherence}\label{ssec:coherence}

We require an information-theoretic measure of how coordinated the components
are.

\begin{definition}[Coherence]\label{def:coherence}
Given a probability distribution $\mu$ over $\Omega$, the \emph{coherence} of
the system is the total correlation (multi-information), defined as the
Kullback--Leibler divergence from the product of marginals:
\begin{equation}\label{eq:coherence}
  \Coh(\mu) = D_{\mathrm{KL}}\!\!\left(\mu \;\Big\|\; \bigotimes_{s \in S} \mu_s\right),
\end{equation}
where $\mu_s$ is the marginal of $\mu$ on $D_s$.  In the discrete case
(or for continuous distributions admitting densities with respect to a
product reference measure), this equals
$\sum_{s \in S} \Ent(\mu_s) - \Ent(\mu)$, where $\Ent$ denotes Shannon
(or differential) entropy.  The KL formulation is preferred because it is
non-negative by construction, invariant under reparametrization, and
well-defined for arbitrary probability measures on Polish spaces.

\end{definition}

\begin{assumption}[Finite coherence regime]\label{asm:finite-coherence}
All stationary measures $\mu^*$ considered satisfy
$\Coh(\mu^*) < \infty$.  This holds automatically in the
discrete case ($|D_s| < \infty$), or whenever $\mu^*$ admits a
density with respect to the product reference measure
$\bigotimes_s \mu^*_s$ (absolute continuity).  On compact
$\Omega$ with continuous dynamics, sufficient regularity is
provided by the weak Feller property and the smoothness of the
exploration kernel.  When $\Coh(\mu) = +\infty$---indicating
maximal statistical dependence---all coherence comparisons should
be read as applying only within the finite-coherence regime.
\end{assumption}

Coherence equals zero if and only if the components are statistically
independent under~$\mu$.  High coherence indicates that the components have
resolved into a coordinated pattern: knowing one component's state constrains
the others.

\begin{remark}[Coherence is a witness, not a standalone criterion]%
\label{rem:coherence-witness}
$\Coh(\mu)$ quantifies statistical coordination under a
distribution~$\mu$, but high $\Coh$ alone does not imply cognition
in the DCR sense.  Cognition requires the
\emph{process}---exploration interleaved with distributed
violation-reducing resolution---and convergence to a
goal-stabilizing regime.  Accordingly, $\Coh$ is used as a
structural \emph{witness} of coordinated stabilization, not as an
isolated definition of cognition.
\end{remark}

\subsection{The Cognitive Triad}\label{ssec:triad}

\begin{definition}[Exploration]\label{def:exploration}
An \emph{exploration kernel} on $\mathcal{N}$ is a Markov kernel
$E$ on $\Omega$ that provides sufficient mixing: we require that
the combined kernel $K = (1-\epsilon)R + \epsilon\,E$
(formalized in \cref{def:combined} below) be
\emph{$\varphi$-irreducible} for some
$\sigma$-finite measure~$\varphi$ on~$\Omega$, i.e., for every
$\omega \in \Omega$ and every measurable set~$A$ with
$\varphi(A) > 0$,
\begin{equation}\label{eq:exploration}
  \sum_{n=1}^{\infty} K^n(\omega, A) > 0.
\end{equation}
We additionally require that $\varphi$ has \emph{full exploration
support}: $\varphi(U) > 0$ for every open set $U \subseteq \Omega$
with $U \cap \mathcal{F}^* \neq \emptyset$
(see \cref{rem:soft-constraints} for~$\mathcal{F}^*$).
Informally: from any configuration, the dynamics can reach any
region of positive $\varphi$-measure in the configuration space,
and in particular any neighborhood of the feasible set.

A convenient sufficient condition: if $E$ admits a transition
density $e(\omega,\omega')$ with respect to a reference measure
$\lambda$ on $\Omega$, and $e(\omega,\omega') \geq m > 0$ for all
$\omega, \omega'$ in some open set $C$ with $\lambda(C) > 0$ that
the chain can reach from any starting state, then $K$ is
$\lambda$-irreducible (see condition~\ref{cond:minorization} for
the matching minorization).  This covers the standard case of
Gaussian or uniform exploration noise on compact subsets
of~$\mathbb{R}^n$.  The convergence results in
\cref{ssec:convergence} use $\varphi$-irreducibility (conditions
\ref{cond:irreducibility} and~\ref{cond:full-support}) directly.
\end{definition}

\begin{definition}[Distributed Constraint Resolution]\label{def:resolution}
A \emph{resolution dynamics} on $\mathcal{N}$ is a Markov kernel
$R$ on $\Omega$ composed from a family of \emph{local update
kernels} $\{R_s\}_{s \in S}$, where each $R_s$ updates only
component~$s$ and depends only on the state of~$s$ and its
neighbors $\mathcal{N}_G(s)$ in~$G$:
\begin{equation}\label{eq:resolution}
  R(\omega, d\omega')
  = \sum_{s \in S} p_s(\omega)\;
    R_s\!\bigl(\omega_{\{s\} \cup \mathcal{N}_G(s)},\, d\omega'_s\bigr)
    \;\delta_{\omega_{-s}}(d\omega'_{-s}),
\end{equation}
where $p_s(\omega) \geq 0$ with $\sum_s p_s = 1$ is an update
schedule (possibly state-dependent) and $\omega_{-s}$ denotes all
components except~$s$.  Each $R_s$ may be deterministic (a point
mass at some $f_s(\omega_{\{s\}\cup\mathcal{N}_G(s)})$) or
stochastic (e.g., a Metropolis step, Glauber conditional, or noisy
local solver); the stochasticity of $R$ combines the randomness
in the schedule $\{p_s\}$ with any randomness within the
individual~$R_s$.  We require that $R$ is violation-reducing in
expectation:
\begin{equation}\label{eq:monotone}
  \int_\Omega \Viol(\omega')\, R(\omega, d\omega')
  \leq \Viol(\omega) \qquad \text{for all } \omega \in \Omega.
\end{equation}
The condition~\cref{eq:monotone} is a requirement on the resolution
kernel $R$ alone (violation is a supermartingale under pure resolution).
The Foster--Lyapunov drift condition~\ref{cond:drift}, stated later for
the \emph{combined} kernel $K = (1-\epsilon)R + \epsilon E$, is a
strictly stronger quantitative condition incorporating both resolution
and exploration; see \cref{rem:drift-sufficient} for how one derives
it from~\cref{eq:monotone}.

The \emph{distributed} qualifier means that the \emph{update rules}
are local: each $R_s$ depends only on the state of~$s$ and its
neighbors.  No central controller evaluates a global objective.
Constraint \emph{satisfaction}, by contrast, is a genuinely
non-local phenomenon: the coherent pattern that emerges involves
coordinated states across the entire network.  The central point of
DCR is precisely this: global coherence arises from purely local
resolution.  A global Lyapunov function $L$ (such as $\Viol + 1$)
may exist as an \emph{analysis tool}---it certifies
convergence---but is not computed or accessed by any component.
\end{definition}

\begin{definition}[Vanishing-Noise Attractor]\label{def:attractor}
A subset $\mathcal{A} \subseteq \Omega$ is a \emph{vanishing-noise
attractor}\footnote{This is \emph{not} the random-dynamical-systems
notion of random attractor; it is the vanishing-noise
stationary-concentration concept used in large-deviations theory.}
for a family of combined DCR dynamics
$\{K_\epsilon\}_{\epsilon > 0}$ if, for each $\epsilon > 0$ for
which $K_\epsilon$ admits at least one stationary distribution
(existence is guaranteed on compact $\Omega$ with weak Feller
kernels; see \cref{lem:stationary}), \emph{every} stationary
distribution $\mu^*$ concentrates near~$\mathcal{A}$: for every
open neighborhood $U \supseteq \mathcal{A}$,
$\mu^*(U) \to 1$ as $\epsilon \to 0$.  (For deterministic
dynamics or vanishing-noise limits, this reduces to the classical
$d(X_t,\mathcal{A}) \to 0$.)

Under the drift and minorization conditions of
\cref{def:combined}, the stationary distribution is unique
(\cref{lem:unique}), so the ``every stationary distribution''
quantifier reduces to the behavior of that unique~$\mu^*_\epsilon$.
The more general form is retained for settings where uniqueness has
not yet been established.
\end{definition}

\begin{definition}[Coherent Attractor]\label{def:stabilization}
A vanishing-noise attractor $\mathcal{A}$ is a \emph{coherent attractor}
(or \emph{goal-stabilizing pattern}) if additionally:
\begin{enumerate}[nosep]
  \item \emph{Coherent}: For at least one stationary distribution
        $\mu^*$ of the dynamics, $\Coh(\mu^*) > 0$.
  \item \emph{Lyapunov-stable in expectation}: There exists a
        measurable function $L : \Omega \to \mathbb{R}_{\geq 0}$
        that achieves its minimum on $\mathcal{A}$ and satisfies
        $\int L\, d(\delta_\omega K) \leq L(\omega)$ outside a
        neighborhood of $\mathcal{A}$ (up to a bounded additive
        constant on a compact set; see condition~\ref{cond:drift}).
\end{enumerate}
\end{definition}

We can now state the central definition:

\begin{definition}[Cognitive System / DCR-System]\label{def:cognitive-system}
A \emph{cognitive system} (or \emph{DCR-system}) is a tuple
$\mathcal{C} = (\mathcal{N}, E, R)$ consisting of a
constraint network $\mathcal{N}$, an exploration kernel $E$
(\cref{def:exploration}), and a resolution dynamics $R$
(\cref{def:resolution}), such that there exists a coherent
attractor $\mathcal{A} \subseteq \Omega$
(\cref{def:stabilization}) for the combined dynamics
$K = (1-\epsilon)R + \epsilon\,E$, i.e., the dynamics converges
in distribution to a stationary measure concentrating
on~$\mathcal{A}$.  When emphasizing the resulting stabilized
regime we write $(\mathcal{N}, E, R; \mathcal{A})$.  We use
``DCR-system'' when emphasizing the formal structure and
``cognitive system'' when invoking the interpretive thesis of
\cref{rem:metaphysical}.
\end{definition}

\begin{remark}
The interplay between exploration and resolution is critical. Pure exploration
without resolution yields noise (high entropy, zero coherence). Pure resolution
without exploration yields rigid, brittle structures that cannot adapt. Cognition
requires both: the system must \emph{explore to discover} and
\emph{resolve to stabilize}.
\end{remark}

\begin{remark}[Equilibrium systems]\label{rem:equilibrium}
An equilibrium Glauber or Metropolis sampler formally satisfies the
DCR axioms (it has constraints, a resolution kernel, and an
exploration kernel).  However, such systems are excluded by the
\emph{interpretive} thesis unless the kernel decomposes into
exploration~$E$ and a \emph{directed} resolution~$R$ that
preferentially reduces violation
(\cref{rem:drift-sufficient}).  A reversible sampler at thermal
equilibrium does not exhibit this directedness:
$\mathbb{E}[\Viol(X_{t+1}) \mid X_t] = \Viol(X_t)$ on average,
and the resolution kernel does not systematically drive the system
toward lower violation.  Cognition in the DCR sense requires
ongoing, directed constraint resolution---not merely
constraint-consistent fluctuation.
\end{remark}

\subsection{Ergodicity and Stationary Concentration}\label{ssec:convergence}

The cognitive triad (\cref{ssec:triad}) is the \emph{structural definition}
of cognition in DCR: it specifies \emph{what} a cognitive system is.  The
conditions below are \emph{technical sufficient conditions} for one natural
class of dynamics implementing the triad---time-homogeneous Markov chains on
compact state spaces---and establish that such implementations converge in
distribution to unique stationary measures concentrating near coherent
attractors.  Other implementations (e.g., continuous-time diffusions,
deterministic cellular automata with noise, agent-based models) may satisfy
the structural definition while requiring different analytical machinery.

\begin{remark}[Why Meyn--Tweedie on compact spaces]\label{rem:meyn-tweedie}
On a compact state space~$\Omega$, existence of a stationary
distribution already follows from the Krylov--Bogoliubov theorem
(tightness is automatic), and one might wonder why we invoke the
heavier Foster--Lyapunov / minorization / Harris recurrence apparatus
of \citet{meyn1993markov}.  Three reasons: (i)~the drift condition
\ref{cond:drift} provides \emph{quantitative} concentration bounds
(\cref{lem:concentration}), not merely existence; (ii)~the
minorization condition \ref{cond:minorization} yields geometric
convergence rates (Theorem~15.0.1 of \citealp{meyn1993markov}),
which inform the timescale separation requirements in
\cref{sec:scale-free}; and (iii)~the framework extends without
modification to non-compact~$\Omega$, which is needed for continuum
limits (\cref{rem:finiteness}) and for physical systems whose natural
state spaces are unbounded (e.g., momentum variables).  The compact
case is the special case where the drift condition is easiest to
verify, not the only case the framework addresses.
\end{remark}

\begin{definition}[Combined DCR Dynamics]\label{def:combined}
Let $\mathcal{N}$ be a constraint network on a compact metrizable configuration
space $\Omega$. A \emph{combined DCR dynamics} is a time-homogeneous Markov
chain $\{X_t\}_{t \geq 0}$ on $\Omega$ with transition kernel
\begin{equation}\label{eq:combined}
  K(\omega, A) = (1 - \epsilon)\, R(\omega, A) + \epsilon\, E(\omega, A),
  \qquad \epsilon \in (0,1),
\end{equation}
where $R$ is the \emph{resolution kernel}
(\cref{def:resolution}) and $E$ is the \emph{exploration kernel}.
We require:
\begin{enumerate}[nosep,label=(\alph*)]
  \item\label{cond:drift} \textbf{Foster--Lyapunov drift.}
    There exist constants $\lambda \in (0,1)$, $b < \infty$, and a
    measurable set $C \subseteq \Omega$ such that for the Lyapunov
    function $L(\omega) = \Viol(\omega) + 1$:
    \begin{equation}\label{eq:drift}
      \int_\Omega L(\omega')\, K(\omega, d\omega') \leq
      \lambda\, L(\omega) + b\, \mathbf{1}_C(\omega),
    \end{equation}
    where $C$ is a \emph{small set} (see condition~\ref{cond:minorization}
    below).  This is the standard Foster--Lyapunov condition of
    \citet{meyn1993markov}, Chapter~11.
  \item\label{cond:irreducibility} \textbf{$\varphi$-irreducibility.}
    There exists a probability measure $\varphi$ on $\Omega$ such that for
    all $\omega \in \Omega$ and all measurable $A$ with $\varphi(A) > 0$:
    \begin{equation}\label{eq:irreducibility}
      \sum_{n=1}^{\infty} K^n(\omega, A) > 0.
    \end{equation}
  \item\label{cond:minorization} \textbf{Minorization on a small set.}
    There exist a measurable set $C \subseteq \Omega$ (the same small set
    as in condition~\ref{cond:drift}), a constant $\delta > 0$, and a
    probability measure $\nu$ on $\Omega$ such that for all
    $\omega \in C$ and all measurable $A$:
    \begin{equation}\label{eq:minorization}
      K(\omega, A) \geq \delta\, \nu(A).
    \end{equation}
    This replaces the classical aperiodicity assumption, which is
    ill-suited to continuous state spaces.  A sufficient condition: the
    exploration kernel $E$ admits a continuous transition density
    $e(\omega,\omega')$ with respect to a reference measure $\lambda$
    on~$\Omega$, and there exists $m > 0$ such that
    $e(\omega,\omega') \geq m$ for all $\omega \in C$,
    $\omega' \in C$.  Then $K(\omega,A) \geq \epsilon\,m\,\lambda(A
    \cap C)$ for $\omega \in C$, which is~\cref{eq:minorization}
    with $\nu = \lambda(\cdot \cap C)/\lambda(C)$ and
    $\delta = \epsilon\,m\,\lambda(C)$.
  \item\label{cond:feller} \textbf{Weak Feller property.} For every bounded
    continuous $g : \Omega \to \mathbb{R}$, the map
    $\omega \mapsto \int g(\omega')\, K(\omega, d\omega')$ is continuous.
  \item\label{cond:full-support} \textbf{Full exploration support.} The
    irreducibility measure $\varphi$ satisfies $\varphi(U) > 0$ for every
    open set $U \subseteq \Omega$ with
    $U \cap \mathcal{F}^* \neq \emptyset$
    (where $\mathcal{F}^* = \mathcal{F}$ or $\mathcal{F}_{\min}$;
    see \cref{rem:soft-constraints}).
\end{enumerate}
\end{definition}

\begin{remark}[Sufficient conditions for the drift]\label{rem:drift-sufficient}
A simple sufficient condition for~\ref{cond:drift} arises when the
resolution and exploration kernels satisfy separate bounds.  Suppose
there exist $\alpha > 0$ and $B < \infty$ such that
(i)~$\int \Viol(\omega')\,R(\omega,d\omega') \leq
(1-\alpha)\,\Viol(\omega)$ for all $\omega \notin \mathcal{F}$, and
(ii)~$\int \Viol(\omega')\,E(\omega,d\omega') \leq B$ for all
$\omega$.  Setting $\lambda = (1-\epsilon)(1-\alpha)$ and decomposing
$K = (1-\epsilon)R + \epsilon E$ gives
\[
  \int L(\omega')\,K(\omega,d\omega')
  \leq \lambda\,L(\omega) + (1-\lambda + \epsilon B),
\]
which is the drift condition~\ref{cond:drift} with
$b = 1-\lambda+\epsilon B$ and
$C = \{\omega : \Viol(\omega) \leq b/(1-\lambda)\}$.
On compact~$\Omega$, condition~(ii) holds automatically
($B = \max_\Omega \Viol < \infty$), so the drift reduces to requiring
that the resolution kernel $R$ contracts violation outside
$\mathcal{F}$.
\end{remark}

\begin{lemma}[Existence of Stationary Distribution]\label{lem:stationary}
Under condition~\ref{cond:feller} of \cref{def:combined} (weak Feller
property), the Markov chain $\{X_t\}$ on compact metrizable~$\Omega$
admits at least one stationary distribution $\mu^*$.
\end{lemma}

\begin{proof}
On a compact metrizable space, tightness of any sequence of
probability measures is automatic.  The Krylov--Bogoliubov theorem
then applies: the Ces\`{a}ro averages
$\mu_T = T^{-1}\sum_{t=0}^{T-1} \delta_\omega K^t$ admit a
weakly convergent subsequence, and the weak Feller
property~\ref{cond:feller} ensures that any weak limit point is a
stationary distribution.  (The drift condition~\ref{cond:drift} is
not needed for existence on compact spaces; it provides the
quantitative concentration bounds in \cref{lem:concentration}.)
\end{proof}

\begin{lemma}[Uniqueness and Ergodicity]\label{lem:unique}
Under conditions \ref{cond:drift}--\ref{cond:full-support} of
\cref{def:combined}, the stationary distribution $\mu^*$ is unique,
and for every initial distribution $\mu_0$:
\begin{equation}\label{eq:ergodic}
  \lVert \mu_0 K^t - \mu^* \rVert_{\mathrm{TV}} \to 0
  \quad \text{as } t \to \infty.
\end{equation}
\end{lemma}

\begin{proof}
Condition~\ref{cond:irreducibility} gives $\varphi$-irreducibility.
Condition~\ref{cond:minorization} provides a $1$-small set~$C$:
$K(\omega, A) \geq \delta\,\nu(A)$ for all $\omega \in C$ and all
measurable~$A$, with $\delta > 0$ and $\nu$ a probability measure
(the minorization holds for $K$ itself at step $n = 1$, not a
higher iterate).
Condition~\ref{cond:drift} provides a Foster--Lyapunov drift to
the small set~$C$.  By Theorem~16.0.1 of
\citet{meyn1993markov}, a $\varphi$-irreducible chain satisfying
a geometric drift condition toward a small set~$C$ is
\emph{geometrically ergodic}: it admits a unique invariant
probability measure~$\mu^*$, and
$\lVert \mu_0 K^t - \mu^* \rVert_{\mathrm{TV}} \leq M\,r^t$ for
constants $M < \infty$, $r < 1$ depending on the initial
distribution.  (The theorem subsumes aperiodicity verification:
$1$-small sets are petite, and drift to a petite set in a
$\varphi$-irreducible chain implies aperiodicity by
Proposition~5.4.5 of \citealp{meyn1993markov}; the geometric
ergodicity theorem bundles these steps.)
\end{proof}

\begin{lemma}[Concentration Near the Feasible Set]\label{lem:concentration}
For fixed $\epsilon > 0$, the unique stationary distribution
$\mu^*_\epsilon$ (\cref{lem:unique}) satisfies
\begin{equation}\label{eq:concentration}
  \int L(\omega)\, \mu^*_\epsilon(d\omega)
  \leq \frac{b}{1 - \lambda},
\end{equation}
where $\lambda, b$ are the drift constants from
condition~\ref{cond:drift}. In particular, when the sufficient
conditions of \cref{rem:drift-sufficient} hold,
$\int \Viol\, d\mu^*_\epsilon \leq
\epsilon B / [\alpha(1-\epsilon) + \epsilon] = O(\epsilon)$.
By Markov's inequality, for any fixed threshold $\delta > 0$:
\begin{equation}\label{eq:markov-concentration}
  \mu^*_\epsilon\bigl(\{\omega : \Viol(\omega) > \delta\}\bigr)
  \leq \frac{O(\epsilon)}{\delta} \;\to\; 0
  \quad \text{as } \epsilon \to 0.
\end{equation}
That is, for any fixed neighborhood of $\mathcal{F}$, the stationary
measure assigns mass tending to~$1$ as $\epsilon \to 0$.  This is
\emph{weak} concentration: it does not imply that $\mu^*_\epsilon$
is supported on an $O(\epsilon)$-neighborhood of $\mathcal{F}$
(which would require exponential tail bounds, e.g., from a spectral
gap or log-Sobolev inequality for the stationary measure).
\end{lemma}

\begin{proof}
Integrating the drift condition~\ref{cond:drift} against the stationary
measure $\mu^*_\epsilon$:
\begin{align*}
  \int L(\omega)\, \mu^*_\epsilon(d\omega)
  &= \int\!\!\int L(\omega')\, K(\omega,d\omega')\, \mu^*_\epsilon(d\omega) \\
  &\leq \lambda \int L(\omega)\, \mu^*_\epsilon(d\omega)
        + b\, \mu^*_\epsilon(C).
\end{align*}
Let $m = \int L\, d\mu^*_\epsilon$. Then
$m \leq \lambda\, m + b$, giving
$m(1 - \lambda) \leq b$, hence \cref{eq:concentration}.
The explicit bound via \cref{rem:drift-sufficient} follows by
substituting $\lambda = (1-\epsilon)(1-\alpha)$ and
$b = 1 - \lambda + \epsilon B$.
\end{proof}

\begin{remark}[Annealing variants]\label{rem:annealing}
The fixed-$\epsilon$ analysis above can be extended to a cooling
schedule $\epsilon_t \to 0$.  If the exploration component is
implemented via a Metropolis--Hastings kernel targeting
$\propto \exp(-\Viol/T_t)$ with a logarithmic cooling schedule
$T_t = c/\log(t+2)$, classical simulated annealing results
\citep{hajek1988cooling} imply convergence of the occupation measures
to a distribution supported on the global minimizers of $\Viol$
(i.e., $\mathcal{F}$), provided the constant $c$ exceeds the
maximum ``depth'' of the energy landscape.  Our convex-mixture kernel
\cref{eq:combined} can be modified to this Metropolis--Hastings form;
the precise annealing analysis of general DCR kernels---which do not
directly match the Metropolis acceptance structure that Hajek's
theorem requires---is an interesting direction that we leave to
future work.

For the present paper, the fixed-$\epsilon$ result
(\cref{thm:convergence}) suffices: it establishes convergence
to a unique ergodic distribution that concentrates near
$\mathcal{F}$, with the concentration sharpening as $\epsilon \to 0$.
\end{remark}

\begin{lemma}[Positive Coherence]\label{lem:coherence}
Let $\mu^*$ be a probability measure on
$\Omega = \prod_{s \in S} D_s$.  If there exist adjacent components
$s, s' \in S$ (with $\{s,s'\} \in E$) such that the $(s,s')$-marginal
of $\mu^*$ is not a product:
\begin{equation}\label{eq:non-product}
  \mu^*_{s,s'} \;\neq\; \mu^*_s \otimes \mu^*_{s'},
\end{equation}
then $\Coh(\mu^*) > 0$.
\end{lemma}

\begin{proof}
By the chain rule for KL divergence (equivalently, the data
processing inequality under marginalization):
\[
  \Coh(\mu^*)
  = D_{\mathrm{KL}}\!\Bigl(\mu^* \;\Big\|\;
    \bigotimes_{r \in S} \mu^*_r\Bigr)
  \;\geq\;
  D_{\mathrm{KL}}\!\bigl(\mu^*_{s,s'} \;\|\;
    \mu^*_s \otimes \mu^*_{s'}\bigr)
  = I_{\mu^*}(s;\, s')
  > 0,
\]
where $I_{\mu^*}(s;\,s')$ is the mutual information between
components $s$ and~$s'$ under~$\mu^*$.  The final strict inequality
holds because $\mu^*_{s,s'} \neq \mu^*_s \otimes \mu^*_{s'}$ by
assumption, and KL divergence vanishes if and only if its arguments
agree.
\end{proof}

\begin{remark}[Why DCR dynamics produce non-product stationary measures]%
\label{rem:coherence-application}
\Cref{lem:coherence} reduces the coherence question to checking
that $\mu^*$ has a non-product marginal along at least one edge.
For DCR dynamics with non-trivial constraints, this holds
generically, by the following argument.

The resolution kernel $R$ updates component~$s$ as a function of
its neighbors (\cref{def:resolution}): after a step updating~$s$,
the new state $X_s^{(t+1)}$ depends on $X_{s'}^{(t)}$ for
$\{s,s'\} \in E$.  Under a uniform random update schedule, such
updates occur with probability $1/|S|$ at each step.  The combined
kernel $K = (1-\epsilon)R + \epsilon E$ therefore introduces
statistical dependence between $s$ and~$s'$ at every step where~$s$
is updated---unless the update rule $f_s$ is constant in the
neighbor states (which would make the constraint structure trivial).

If the unique stationary distribution $\mu^*_\epsilon$ were a
product $\bigotimes_r \mu^*_r$, then components $s$ and~$s'$ would
be independent under~$\mu^*_\epsilon$.  But the resolution dynamics
makes $s$'s next-step distribution depend on the current state
of~$s'$, so a product stationary measure would require this
neighbor-dependence to be exactly cancelled by the marginal
averaging---a non-generic coincidence that fails whenever the
constraint relation $R_{\{s,s'\}}$ is non-trivial and the
exploration noise does not fully decorrelate in one step (which
$\epsilon < 1$ guarantees).

In the finite-state case (\cref{ex:ising}), the non-product
structure can be verified directly by computing $\mu^*_\epsilon$
from the transition matrix.  In continuous-state cases
(\cref{ex:benard}), the analogous statement follows from the fact
that the Gauss--Seidel update of parcel~$s$ depends continuously on
the states of its lattice neighbors, so the joint $(s,s')$-marginal
under~$\mu^*_\epsilon$ inherits this dependence.

\textbf{Caveat.}  The above is a heuristic argument, not a theorem.
Counterexamples exist: certain reversible Markov chains satisfying
detailed balance with respect to a product Gibbs measure have
neighbor-dependent update rules yet product stationary distributions.
In each application, the non-product property of $\mu^*_\epsilon$
must be verified for the specific dynamics at hand---either by direct
computation (finite-state case) or by checking that the update
rule and noise structure preclude the detailed-balance cancellation
that would yield a product measure.
\end{remark}

\begin{remark}[Heuristic criterion for non-product stationary
measure]\label{rem:non-product-heuristic}
In practice, the non-product hypothesis~\cref{eq:non-product-hyp}
holds whenever the resolution kernel introduces genuine dependence
between neighbors and the exploration kernel does not wash it out.
Concretely, if $K$ is irreducible on a finite state space,
$R_s(\cdot \mid a, b) \neq R_s(\cdot \mid a, b')$ for some
neighbor pair $\{s,s'\}$ and some states $b \neq b'$, and $E$
does not fully mix in one step (i.e., $E$ is not a constant
kernel), then the stationarity equation for the $(s,s')$-marginal
forces a cancellation between the dependence introduced by $R$ and
the mixing by $E$, which generically fails.  However, contrived
counterexamples exist (cf.\ \cref{rem:coherence-application}), so
we state~\cref{eq:non-product-hyp} as an explicit hypothesis in
\cref{thm:convergence} rather than deriving it from these
conditions alone.
\end{remark}

\begin{proposition}[Non-product for Gibbs interactions]%
\label{prop:non-product-gibbs}
Let $\Omega = \prod_s D_s$ be a finite state space with pairwise
interaction Gibbs measure
$\pi(\omega) \propto \exp\!\bigl(-\sum_{\{s,s'\} \in E}
J_{s,s'}(\omega_s, \omega_{s'})\bigr)$,
and let $R$ be reversible with respect to $\pi$ (e.g., Metropolis
or Glauber dynamics).  Let $K = (1-\epsilon)R + \epsilon\,E$ with
$K$ irreducible.  If $J_{s,s'} \not\equiv \mathrm{const}$ for some
edge $\{s,s'\}$, then for all sufficiently small $\epsilon > 0$
the unique stationary distribution $\mu^*_\epsilon$ satisfies
$(\mu^*_\epsilon)_{s,s'} \neq (\mu^*_\epsilon)_s \otimes
(\mu^*_\epsilon)_{s'}$.
\end{proposition}

\begin{proof}
At $\epsilon = 0$ the unique stationary distribution is $\pi$
itself, which is non-product whenever $J_{s,s'} \not\equiv
\mathrm{const}$ (this is a standard property of Gibbs measures
with non-trivial interactions).  For $\epsilon > 0$,
$\mu^*_\epsilon$ depends continuously on~$\epsilon$ (by
perturbation theory for finite irreducible chains).  Since the set
of product distributions is closed, $\mu^*_\epsilon$ remains
non-product for all sufficiently small~$\epsilon$.
\end{proof}

We can now state and prove the main convergence theorem:

\begin{theorem}[Convergence of DCR Dynamics]\label{thm:convergence}
Let $\mathcal{N}$ be a constraint network with compact configuration space
$\Omega$ and non-empty $\mathcal{F}^*$
(\cref{rem:soft-constraints}). Let the constraint graph $G$ be
connected.

Let $\{X_t\}$ be a combined DCR dynamics (\cref{def:combined}) with fixed
$\epsilon \in (0,1)$ satisfying conditions
\ref{cond:drift}--\ref{cond:full-support}, and suppose that the
unique stationary distribution $\mu^*_\epsilon$ (\cref{lem:unique})
satisfies the following non-degeneracy condition:
\begin{equation}\label{eq:non-product-hyp}\tag{H1}
  \textbf{Non-degeneracy of stationary coupling:}\quad
  \exists\, \{s,s'\} \in E : \quad
  (\mu^*_\epsilon)_{s,s'} \;\neq\;
  (\mu^*_\epsilon)_s \otimes (\mu^*_\epsilon)_{s'}.
\end{equation}
(A heuristic criterion for~\ref{eq:non-product-hyp} is discussed
in \cref{rem:coherence-application}; a rigorous sufficient condition
for the Gibbs class is given by \cref{prop:non-product-gibbs}.
In general, \ref{eq:non-product-hyp} must be verified for specific
dynamics.) Then:
\begin{enumerate}[nosep,label=(\roman*)]
  \item The chain converges to a unique stationary distribution
    $\mu^*_\epsilon$ with
    $\mathbb{E}_{\mu^*_\epsilon}[\Viol] = O(\epsilon)$
    (\cref{lem:concentration}).  For any fixed $\delta > 0$,
    $\mu^*_\epsilon(\Viol > \delta) \to 0$ as $\epsilon \to 0$
    (\cref{eq:markov-concentration}): the stationary measure
    concentrates near $\mathcal{F}$ in the weak sense.
  \item $\Coh(\mu^*_\epsilon) > 0$.
  \item $\mathcal{F}$ is a goal-stabilizing pattern with Lyapunov
    function $L = \Viol + 1$: the drift condition ensures
    $L$ is decreasing in expectation outside any sublevel set, the
    ergodic distribution concentrates near $\mathcal{F}$
    (\cref{eq:markov-concentration}), and $\Coh(\mu^*_\epsilon) > 0$
    by~(ii).  For fixed $\epsilon > 0$ the chain is ergodic with full
    support (due to the exploration kernel), so the ``attraction'' is
    stochastic: the chain spends most of its time near $\mathcal{F}$,
    with the fraction increasing as $\epsilon \to 0$.
\end{enumerate}
\end{theorem}

\begin{proof}
By condition~\ref{cond:drift}, the chain satisfies a Foster--Lyapunov drift
condition (see \cref{rem:drift-sufficient} for sufficient conditions).
By \cref{lem:stationary}, a stationary distribution $\mu^*_\epsilon$ exists.
By \cref{lem:unique}, $\mu^*_\epsilon$ is unique and the chain is ergodic.
By \cref{lem:concentration}, $\mathbb{E}_{\mu^*_\epsilon}[\Viol] =
O(\epsilon)$, and \cref{eq:markov-concentration} gives weak
concentration near $\mathcal{F}$.
By the non-product hypothesis~\cref{eq:non-product-hyp},
$\Coh(\mu^*_\epsilon) > 0$ by \cref{lem:coherence}.

$L(\omega) = \Viol(\omega) + 1$ is continuous and bounded on
compact $\Omega$, achieves its minimum on $\mathcal{F}$, and
satisfies the drift condition~\ref{cond:drift}.  The ergodic theorem
gives $T^{-1}\sum_{t=0}^{T-1} L(X_t) \to \int L\,d\mu^*_\epsilon
= O(\epsilon)$ a.s., confirming that the time-averaged violation
is small.  Since $\mu^*_\epsilon$ has full support on $\Omega$
(the exploration kernel ensures this), the chain does not converge
to $\mathcal{F}$ in the pathwise sense $d(X_t,\mathcal{F}) \to 0$;
rather, $\mathcal{F}$ is the high-probability region of the ergodic
distribution, and the concentration sharpens as $\epsilon \to 0$.

For convergence under cooling schedules (where pathwise convergence
to $\mathcal{F}$ can be recovered), see \cref{rem:annealing}.
\end{proof}

\subsection{What DCR Excludes}\label{ssec:exclusions}

A definition of cognition is only useful if it excludes something. DCR excludes
three classes of systems:

\begin{enumerate}
  \item \textbf{Equilibrium systems.} A system at thermodynamic equilibrium
        occupies a maximum-entropy state with no net flows. There is no
        directed exploration--resolution dynamics: the system does not
        undergo a constraint-reducing process converging toward a coherent
        attractor---it is already at the fixed point of entropy
        maximization.  (An interacting equilibrium system, e.g., an Ising
        model below $T_c$, may have high statistical correlations, but
        these correlations are not \emph{maintained by} an ongoing
        explore--resolve--stabilize cycle; they are a static consequence
        of the Hamiltonian.  DCR requires the \emph{process}, not merely
        the correlations.)
        \emph{A rock at thermal equilibrium is not cognitive.}

  \item \textbf{Unconstrained stochastic systems.} A system with exploration
        but no constraint structure ($\mathcal{R} = \emptyset$) undergoes a
        random walk on $\Omega$ with no convergence to coherent patterns.
        $\Viol \equiv 0$ trivially, and $\Coh(\mu) = 0$ for the stationary
        (uniform) distribution. \emph{Brownian motion in free space is not
        cognitive.}

  \item \textbf{Fully deterministic single-trajectory systems.} A system with
        a single degree of freedom following a deterministic trajectory has no
        exploration (the path is unique) and no distributed resolution (there
        is only one component). \emph{A single classical particle in a
        potential well is not cognitive.}
\end{enumerate}

Cognition in the DCR sense requires the non-trivial intersection:
non-equilibrium dynamics, constraint structure, and distributed convergence
to coherent attractors.

We emphasize that the boundary between cognitive and non-cognitive is
\emph{continuous}, not sharp. A system near equilibrium with weak
constraints and small fluctuations satisfies the DCR conditions only
marginally: the coherence $\Coh(\mu^*)$ is near zero and the convergence
timescale is long. DCR does not impose a binary threshold; rather, it
provides a graded measure through the coherence of the attractor and the
cognitive depth $\delta$ (\cref{def:depth}). The exclusions above identify
the \emph{limiting cases} where one or more components of the triad vanish
entirely, not a boundary that systems cross discontinuously.

\subsection{Worked Example: Antiferromagnetic Ising Model}%
\label{ssec:ising-example}

Before proceeding to scale-freeness and physics, we ground the
framework in a concrete toy model where every quantity can be computed
explicitly.

\begin{example}[Antiferromagnetic Ising lattice]\label{ex:ising}
Consider $N$ spins on a graph $G = (S, E)$ (e.g., a $d$-dimensional
lattice) with $D_s = \{-1, +1\}$ for each $s \in S$.  This is a
finite constraint network with
$\Omega = \{-1,+1\}^N$ ($|\Omega| = 2^N$).

\textbf{Constraints.}
The antiferromagnetic coupling prefers opposite spins on neighboring
sites.  For each edge $e = \{s,s'\} \in E$:
\[
  v_e(\omega_s, \omega_{s'})
  = \tfrac{1}{2}(1 + \omega_s\,\omega_{s'})
  = \begin{cases}
      0 & \text{if } \omega_s \neq \omega_{s'},\\
      1 & \text{if } \omega_s = \omega_{s'}.
    \end{cases}
\]
The total violation is
$\Viol(\omega) = \sum_{\{s,s'\} \in E} v_e(\omega_s,\omega_{s'})$,
which counts the number of frustrated (same-spin) edges.  The
feasible set $\mathcal{F} = \{\omega : \Viol(\omega) = 0\}$ consists
of the proper 2-colorings of~$G$ (if $G$ is bipartite) or is empty
(if $G$ is not bipartite; in the latter case one works with
approximate feasibility).

\textbf{Resolution kernel.}
Select a site $s$ uniformly at random. If flipping $s$ strictly
reduces $\Viol$, flip it; otherwise keep it.  This defines a Markov
kernel $R$ satisfying $\int \Viol(\omega')\,R(\omega,d\omega')
\leq \Viol(\omega)$ for all~$\omega$ (violation-reducing,
\cref{def:resolution}).  Updates are local: the flip decision for~$s$
depends only on $\omega_s$ and $(\omega_{s'})_{s' \in
\mathcal{N}_G(s)}$.

\textbf{Exploration kernel.}
With probability $\epsilon$, flip a uniformly random site regardless
of energy change: $E(\omega, \omega') = (1/N)\,\mathbf{1}[\omega'
\text{ differs from } \omega \text{ at exactly one site}]$ (plus a
self-loop with appropriate weight).  Since any configuration is
reachable from any other via a sequence of single-site flips,
$E$ is $\varphi$-irreducible with $\varphi$ the counting measure.

\textbf{DCR conditions.}
\begin{itemize}[nosep]
  \item \emph{Irreducibility and aperiodicity:}
    The exploration kernel $E$ connects any pair of configurations
    at Hamming distance~$1$, so the combined kernel $K$ makes the
    state graph on $\{-1,+1\}^N$ strongly connected (any
    configuration is reachable via a sequence of single-site
    flips) and $K(\omega, \omega) > 0$ (self-loops from the
    resolution step doing nothing).  On a finite state space, an
    irreducible aperiodic Markov chain has a unique stationary
    distribution and converges geometrically in total variation
    (the Perron--Frobenius theorem).
    The Foster--Lyapunov drift framework of
    \cref{ssec:convergence} is not needed here; it becomes
    essential for continuous state spaces (\cref{ex:benard})
    where finite-state arguments are unavailable.
  \item \emph{Violation-reducing resolution:}
    The greedy flip rule satisfies~\cref{eq:monotone}
    ($\mathbb{E}[\Viol(X_{t+1})] \leq \Viol(X_t)$ under $R$
    alone).  Note that $R$ may have local minima (configurations
    where no single-site flip reduces $\Viol$); the exploration
    kernel $E$ ensures the chain escapes these.
  \item \emph{Non-product marginal:}
    The Ising model is a pairwise Gibbs measure with
    $J_{s,s'}(\omega_s,\omega_{s'}) = -J\,\omega_s\omega_{s'}$
    and $J \neq 0$.  The Metropolis update $R$ is reversible
    with respect to this Gibbs measure.
    By \cref{prop:non-product-gibbs},
    $(\mu^*_\epsilon)_{s,s'} \neq
    (\mu^*_\epsilon)_s \otimes (\mu^*_\epsilon)_{s'}$
    for sufficiently small~$\epsilon$.
\end{itemize}

\textbf{Result.}
By \cref{thm:convergence}, the chain converges to a unique
$\mu^*_\epsilon$ with $\Coh(\mu^*_\epsilon) > 0$ (the spins are
correlated under the stationary distribution---knowing one spin's
state constrains its neighbor's).  As $\epsilon \to 0$, the
stationary measure concentrates on the absorbing configurations of
$R$ (local minima of $\Viol$); on bipartite graphs, these are
exactly the ground states in $\mathcal{F}$, so
$\mu^*_\epsilon(\mathcal{F}) \to 1$.  This is a cognitive system:
the spins explore configurations via random flips, resolve
constraints by locally reducing frustration, and stabilize into a
coherent antiferromagnetic pattern.  It is cognitive of depth~1.
\end{example}

\begin{remark}[Relation to optimization and computer science]%
\label{rem:optimization-cs}
The Ising example makes explicit the connection between DCR and
classical optimization/constraint-satisfaction frameworks.
\emph{Simulated annealing}
\citep{hajek1988cooling} is a DCR dynamics where $\epsilon$
(temperature) is decreased over time, driving the system toward
global violation minimizers.  \emph{Belief propagation} and
message-passing algorithms for random constraint satisfaction
problems \citep{mezard2002random} implement distributed resolution:
each variable node updates its marginal based on neighboring
constraints, a process structurally identical to the resolution
kernel~$R$.  More broadly, any local-search heuristic for
combinatorial constraint satisfaction (SAT, CSP, graph coloring)
instantiates the exploration--resolution interplay.  DCR does not
claim novelty in these algorithms; it claims that the
\emph{same formal triad} appears in physical, biological, and
cognitive systems, not only in engineered solvers.
\end{remark}

% ==========================================================================
\section{Scale-Freeness and the Combination Problem}\label{sec:scale-free}
% ==========================================================================

The central mathematical contribution of this paper is showing that DCR is
closed under coarse-graining: cognitive systems at one scale compose into
cognitive systems at the next, providing a constructive account of
composition addressing the combination problem.

The closure result (\cref{thm:closure}) requires \emph{timescale separation}
between intra-group and inter-group dynamics---a condition that holds in many
physical systems (atomic vs.\ molecular, synaptic vs.\ network) but is not
universal. We regard this as a sufficient condition, not a necessary one;
weakening it to overlapping timescales or continuous-time limits is an
important open problem (see \cref{sec:discussion}). The framework's axioms
(\cref{sec:framework}) are themselves scale-free; it is specifically the
composition mechanism that requires separation.

\subsection{The Coarse-Graining Construction}\label{ssec:coarse-graining}

\begin{definition}[Coarse-Graining]\label{def:coarse-graining}
Let $\mathcal{C} = (\mathcal{N}, \{X_t\}, R, \mathcal{A})$ be a
cognitive system with component set $S$, and let
$\pi : S \twoheadrightarrow S'$ be a surjective \emph{partition map}
grouping components into macro-components. A \emph{coarse-graining}
$\Gamma_{\pi,g}(\mathcal{C})$ additionally requires \emph{compression
maps} $\{g_{s'}\}_{s' \in S'}$ and is constructed as follows:

\begin{enumerate}[nosep]
  \item \textbf{Compression maps:} For each $s' \in S'$, a measurable
        surjection $g_{s'} : \prod_{s \in \pi^{-1}(s')} D_s \to
        \tilde{D}_{s'}$ onto a measurable \emph{macro-state space}
        $\tilde{D}_{s'}$, typically of lower dimension than the group
        state space $\prod_{s \in \pi^{-1}(s')} D_s$. The global
        compression
        $g : \Omega \to \tilde{\Omega} = \prod_{s'} \tilde{D}_{s'}$ is
        defined by $g(\omega)_{s'} = g_{s'}(\omega_{\pi^{-1}(s')})$.
  \item \textbf{Macro-components:} $S' = \pi(S)$, with state spaces
        $\tilde{D}_{s'}$.
  \item \textbf{Macro-constraints:} An edge $\{s'_1, s'_2\} \in E'$ exists
        whenever there exist $s_1 \in \pi^{-1}(s'_1)$ and
        $s_2 \in \pi^{-1}(s'_2)$ with $\{s_1, s_2\} \in E$. The
        macro-constraint
        $\tilde{R}_{\{s'_1,s'_2\}} \subseteq \tilde{D}_{s'_1} \times
        \tilde{D}_{s'_2}$ consists of all macro-state pairs
        $(\tilde{\omega}_{s'_1}, \tilde{\omega}_{s'_2})$ for which there
        exist group configurations in the preimages
        $g_{s'_j}^{-1}(\tilde{\omega}_{s'_j})$ satisfying all cross-group
        micro-constraints between $\pi^{-1}(s'_1)$ and $\pi^{-1}(s'_2)$.
        Once the macro-violation $\tilde{v}_{e'}$ is available
        (\cref{def:macro-violation}), the effective macro-constraint is
        its zero set:
        $\tilde{R}_{\{s'_1,s'_2\}} = \{(\tilde{\omega}_{s'_1},
        \tilde{\omega}_{s'_2}) :
        \tilde{v}_{\{s'_1,s'_2\}}(\tilde{\omega}_{s'_1},
        \tilde{\omega}_{s'_2}) = 0\}$; the existential definition above
        serves only to establish the edge set $E'$.
  \item \textbf{Macro-dynamics (effective kernel).}
        Let $t_k$ index the times of inter-group updates (slow
        timescale).  Define the macro-state
        $\tilde{X}_k := g(X_{t_k})$.  Under $\eta$-timescale
        separation (\cref{def:timescale}), $\{\tilde{X}_k\}_{k \geq 0}$
        is (exactly in the limit $\eta\tau_{\text{fast}} \to 0$, or
        approximately for finite separation) a time-homogeneous Markov
        chain on $\tilde{\Omega}$ with effective transition kernel
        $\tilde{K}$ obtained by averaging the micro-transition across
        the conditional equilibria
        $\nu_{s'}(\cdot \mid b_{s'})$---see
        \cref{lem:effective-markov}.
  \item \textbf{Macro-attractor:}
        $\tilde{\mathcal{A}} = g(\mathcal{A})$, or more generally
        (in the soft-constraint case, \cref{rem:soft-constraints})
        $\tilde{\mathcal{F}}_{\min} :=
        \arg\min_{\tilde{\omega}} \widetilde{\Viol}(\tilde{\omega})$.
\end{enumerate}
Whenever any $g_{s'}$ is non-injective---the generic and intended
case---the compression discards degrees of freedom: $\tilde{\Omega}$
carries strictly less information than $\Omega$ (and in
Euclidean-state cases, has lower dimension). Concrete choices of
$g_{s'}$ include empirical averages (mean-field), order parameters,
sufficient statistics, or information-bottleneck summaries; the
appropriate choice is determined by the physics of the system and the
timescale separation structure (\cref{def:timescale}).

We assume throughout that all state spaces $D_s$ (and hence
$\tilde{D}_{s'}$) are Polish (complete separable metrizable) and that
all maps and constraint relations are Borel. Under these standing
assumptions, images and projections of Borel sets are analytic and
hence universally measurable. In particular,
$\tilde{\mathcal{R}}$, $\tilde{\mathcal{F}}$, and the pushforward
measures arising in the construction are universally measurable
(though not Borel in general).
\end{definition}

We now state the conditions precisely and prove closure through three lemmas,
one for each component of the cognitive triad.

\begin{definition}[Timescale Separation]\label{def:timescale}
Let $\eta > 0$ be a \emph{scale ratio}. We say the micro-dynamics exhibits
$\eta$-\emph{timescale separation} with respect to partition $\pi$ and
compression $\{g_{s'}\}$ if:
\begin{enumerate}[nosep]
  \item \textbf{Fast intra-group mixing.} For each macro-component
    $s' \in S'$, let $\partial s'$ denote the micro-components outside
    $\pi^{-1}(s')$ that share an edge with some component inside
    $\pi^{-1}(s')$ (the \emph{micro-boundary} of group $s'$). For each
    boundary configuration $b_{s'} = (\omega_s)_{s \in \partial s'}$,
    the restricted chain on $\prod_{s \in \pi^{-1}(s')} D_s$ with
    $b_{s'}$ frozen has a unique conditional equilibrium
    $\nu_{s'}(\cdot \mid b_{s'})$ and mixes to this equilibrium in time
    $\tau_{\text{fast}}$.
  \item \textbf{Slow inter-group dynamics.} The inter-group updates (those
    involving edges in $E_{\text{cross}} = \{\{s_1,s_2\} \in E \mid
    \pi(s_1) \neq \pi(s_2)\}$) occur at rate $\eta$ relative to intra-group
    updates.
  \item $\eta\, \tau_{\text{fast}} \to 0$, i.e., fast degrees of freedom
    equilibrate before the next inter-group update.
  \item \textbf{Macro-sufficiency (representative-independence).}
    Define the \emph{macro-boundary} of group $s'$ as
    $\tilde{b}_{s'}(\tilde{\omega}) =
    (\tilde{\omega}_{s''})_{s'' \in \mathcal{N}_{G'}(s')}$.
    For each global micro-configuration $\omega \in \Omega$ write
    $b_{s'}(\omega) := (\omega_s)_{s \in \partial s'}$ for the induced
    micro-boundary.  We require that for any two micro-configurations
    $\omega, \omega' \in \Omega$ and any inter-group edge
    $\{s_1, s_2\} \in E_{\text{cross}}$ with $\pi(s_j) = s'_j$,
    if $\tilde{b}_{s'_1}(g(\omega)) = \tilde{b}_{s'_1}(g(\omega'))$
    and
    $\tilde{b}_{s'_2}(g(\omega)) = \tilde{b}_{s'_2}(g(\omega'))$,
    then
    \[
      \mathbb{E}_{\nu_{s'_1}(\cdot \mid b_{s'_1}(\omega)) \otimes
      \nu_{s'_2}(\cdot \mid b_{s'_2}(\omega))}
      \bigl[v_{\{s_1,s_2\}}\bigr]
      =
      \mathbb{E}_{\nu_{s'_1}(\cdot \mid b_{s'_1}(\omega')) \otimes
      \nu_{s'_2}(\cdot \mid b_{s'_2}(\omega'))}
      \bigl[v_{\{s_1,s_2\}}\bigr].
    \]
\end{enumerate}
Condition~4 says that the compression maps capture all boundary
information relevant to inter-group interactions: two global
micro-configurations that agree on the macro-neighborhood of both
endpoint groups yield the same expected cross-group violation after
fast equilibration, even if their micro-boundaries differ in detail
not captured by $g$.
This is what makes $\tilde{v}_{e'}$ in \cref{def:macro-violation}
well-defined as a function on $\tilde{D}_{s'_1} \times
\tilde{D}_{s'_2}$.  We take macro-sufficiency as a sufficient
condition ensuring well-defined macro-violations; weaker approximate
forms (e.g., $\epsilon$-lumpability, where the displayed equality holds
up to an error of order~$\epsilon$) are natural relaxations that we
leave to future work.
\end{definition}

\begin{remark}[Relation to lumpability and conditional independence]%
\label{rem:lumpability}
Condition~4 of \cref{def:timescale} is a form of (approximate)
\emph{lumpability}: the compressed macrostate renders micro-boundary
details conditionally irrelevant for inter-group interactions after
fast equilibration.  This parallels conditional independence
assumptions used in renormalization group methods, Mori--Zwanzig
averaging, and Markov state modelling.  In practice, the condition
will hold approximately rather than exactly; the error term
$\varepsilon(\eta, \rho)$ in \cref{lem:effective-markov}
quantifies the approximation.  We emphasize that the closure theorem
(\cref{thm:closure}) is conditional on this assumption: without
timescale separation and macro-sufficiency, the coarse-graining
construction may not yield a cognitive system at the macro level.
DCR still applies at each scale separately; it is specifically the
\emph{composition} across scales that requires these conditions.
Absent (approximate) lumpability, the coarse-grained process is
generally non-Markovian; our result isolates conditions under which
Markovian closure holds.
\end{remark}

\begin{definition}[Macro-Violation]\label{def:macro-violation}
Given the conditional equilibria from \cref{def:timescale}, the
\emph{macro-violation} of a macro-configuration
$\tilde{\omega} = (\tilde{\omega}_{s'})_{s' \in S'} \in \tilde{\Omega}$
with respect to an inter-group edge $e' = \{s'_1, s'_2\} \in E'$ is
\begin{equation}\label{eq:macro-violation}
  \tilde{v}_{e'}(\tilde{\omega}_{s'_1}, \tilde{\omega}_{s'_2}) =
  \sum_{\substack{\{s_1,s_2\} \in E_{\text{cross}} \\
  \pi(s_1) = s'_1,\, \pi(s_2) = s'_2}}
  \mathbb{E}_{\nu_{s'_1}(\cdot\,|\,b_{s'_1}) \otimes
  \nu_{s'_2}(\cdot\,|\,b_{s'_2})}
  \bigl[v_{\{s_1,s_2\}}(x_{s_1}, x_{s_2})\bigr],
\end{equation}
where $\omega \in \Omega$ is any micro-configuration satisfying
$g(\omega) = \tilde{\omega}$ and $b_{s'_j} = b_{s'_j}(\omega)$ denotes
the induced micro-boundary of group~$s'_j$.
The total macro-violation is
$\widetilde{\Viol}(\tilde{\omega}) = \sum_{e' \in E'}
\tilde{v}_{e'}$. By the representative-independence condition
(\cref{def:timescale}, item~4), the right-hand side of
\cref{eq:macro-violation} does not depend on the choice of~$\omega$,
so $\tilde{v}_{e'}$ is well-defined as a function on
$\tilde{D}_{s'_1} \times \tilde{D}_{s'_2}$.  Since the definition
is independent of the representative, no measurable selection from
$g^{-1}(\tilde{\omega})$ is needed; $\tilde{v}_{e'}$ is a
universally measurable function on the macro-state spaces (as a
composition of measurable maps and integration against the
conditional equilibria).
\end{definition}

\begin{lemma}[Effective Markovianity Under Timescale Separation]%
\label{lem:effective-markov}
Under $\eta$-timescale separation and macro-sufficiency
(\cref{def:timescale}, items~1--4), and additionally assuming:
\begin{enumerate}[nosep,label=(M\arabic*)]
  \item\label{cond:uniform-mixing} \textbf{Uniform geometric
    ergodicity of the fast chain.}  For each group $s'$ and boundary
    configuration $b_{s'}$, the restricted intra-group chain mixes
    to $\nu_{s'}(\cdot \mid b_{s'})$ at a geometric rate
    $\rho < 1$ uniformly over $b_{s'}$: the total variation
    distance decays as $O(\rho^n)$ after $n$ intra-group steps.
  \item\label{cond:lifting-regularity} \textbf{Lifting kernel
    regularity.}  The product of conditional equilibria
    $\mu_{\tilde{\omega}} := \bigotimes_{s'} \nu_{s'}(\cdot \mid
    \tilde{b}_{s'}(\tilde{\omega}))$ on $\Omega$ admits a
    \emph{regular conditional probability} (disintegration) with
    respect to the compression~$g$:
    $\mu_{\tilde{\omega}}(d\omega) = \rho_{\tilde{\omega}'}
    (d\omega)\, (g_\# \mu_{\tilde{\omega}})(d\tilde{\omega}')$,
    where $\rho_{\tilde{\omega}'}$ is a probability measure
    concentrated on the fiber
    $g^{-1}(\tilde{\omega}')$.  The lifting measure
    $\rho_{\tilde{\omega}}$ used in~\cref{eq:macro-kernel} is
    $\rho_{\tilde{\omega}'}\big|_{\tilde{\omega}'=\tilde{\omega}}$.
    On standard Borel spaces (which includes all compact metrizable
    $\Omega$), regular conditional probabilities exist
    unconditionally; we require additionally that the map
    $\tilde{\omega} \mapsto \rho_{\tilde{\omega}}$ is measurable
    in the weak topology.
\end{enumerate}

\noindent
Define the effective macro transition kernel:
\begin{equation}\label{eq:macro-kernel}
  \tilde{K}(\tilde{\omega}, \tilde{A})
  = \int_{\Omega}
    K_{\text{slow}}(\omega, g^{-1}(\tilde{A}))\;
    \rho_{\tilde{\omega}}(d\omega),
\end{equation}
where $K_{\text{slow}}$ is the micro-kernel restricted to inter-group
updates.

Then, in the limit $\eta\tau_{\text{fast}} \to 0$, the slow-time
macro-process $\{\tilde{X}_k\}_{k \geq 0}$ defined at inter-group
update times is a time-homogeneous Markov chain on $\tilde{\Omega}$
with kernel~$\tilde{K}$.  For finite timescale separation,
$\{\tilde{X}_k\}$ is approximately Markov: there exists
$\varepsilon(\eta, \rho) \to 0$ as
$\eta\tau_{\text{fast}} \to 0$ such that
\begin{equation}\label{eq:approx-markov}
  \sup_{\tilde{A}}\bigl|
    \Pr[\tilde{X}_{k+1} \in \tilde{A} \mid
      \tilde{X}_0,\ldots,\tilde{X}_k]
    - \tilde{K}(\tilde{X}_k, \tilde{A})
  \bigr| \leq \varepsilon(\eta, \rho)
  \quad \text{a.s.,}
\end{equation}
where $\rho < 1$ is the uniform geometric mixing rate
from~\ref{cond:uniform-mixing}.  (Under the continuous-time
analogue, singular perturbation results give
$\varepsilon = O(\eta\,\tau_{\text{fast}})$; in the discrete-time
setting the rate depends on $\rho$ and the state-space dimension,
and we do not establish it here.)
\end{lemma}

\begin{proof}[Proof sketch]
Under timescale separation (condition~3), each group equilibrates to
its conditional equilibrium $\nu_{s'}(\cdot \mid b_{s'})$ before the
next inter-group update.  By uniform geometric
mixing~\ref{cond:uniform-mixing}, the conditional distribution of the
micro-state given the macro-state converges to the disintegration
measure $\rho_{\tilde{\omega}}$ (\ref{cond:lifting-regularity}) at
rate $O(\rho^{1/\eta})$ as $\eta\tau_{\text{fast}} \to 0$.  By
macro-sufficiency (condition~4), $\rho_{\tilde{\omega}}$ depends on
the micro-boundary only through the macro-boundary
$\tilde{b}_{s'}(\tilde{\omega})$, which is determined by
$\tilde{\omega}$ alone.  Hence, in the limit, the one-step transition
probability of~$\tilde{X}_{k+1}$ given
$\tilde{X}_0, \ldots, \tilde{X}_k$ depends only on $\tilde{X}_k$: the
process is Markov with kernel $\tilde{K}$ given by
\cref{eq:macro-kernel}.

For finite separation, the intra-group distribution at the next
inter-group update time differs from $\rho_{\tilde{\omega}}$ by
$\varepsilon(\eta, \rho)$ in total variation (by the geometric
mixing bound from~\ref{cond:uniform-mixing}),
yielding~\cref{eq:approx-markov}.

We label this a ``proof sketch'' because a fully rigorous treatment
would require a discrete-time Khasminskii-type averaging argument
with explicit control of the approximation error's dependence on
the state space dimension and the macro-state.  The continuous-time
analogue is standard (see, e.g., the averaging principle for
singularly perturbed Markov chains); the discrete-time version under
our specific assumptions is a straightforward adaptation that we do
not develop in full detail here.
\end{proof}

\begin{lemma}[Macro-Resolution Inherits Drift]\label{lem:macro-drift}
Under the assumptions of \cref{lem:effective-markov}, and additionally
assuming:
\begin{enumerate}[nosep,label=(D\arabic*)]
  \item\label{cond:inter-contraction} \textbf{Inter-group contraction.}
    There exists $\alpha > 0$ such that for each inter-group edge
    $\{s_1,s_2\} \in E_{\text{cross}}$, the expected cross-group
    violation after one inter-group update, conditioned on local
    equilibria within each group, contracts by at least
    $(1-\alpha)$: i.e., the inter-group resolution kernel reduces
    $\tilde{v}_{e'}$ in expectation uniformly over
    $\tilde{\omega}$.
\end{enumerate}

\noindent
Then the effective macro kernel $\tilde{K}$
(\cref{lem:effective-markov}) satisfies a Foster--Lyapunov drift
condition for the macro-Lyapunov function
$\tilde{L}(\tilde{\omega}) = \widetilde{\Viol}(\tilde{\omega}) + 1$.
\end{lemma}

\begin{proof}[Proof sketch]
Decompose the total micro-violation into intra-group and inter-group parts:
$\Viol(\omega) = \Viol_{\text{intra}}(\omega) + \Viol_{\text{inter}}(\omega)$.
Under timescale separation, the fast dynamics drives
$\Viol_{\text{intra}} \to 0$ within each group (by the micro-level drift
condition applied to intra-group edges). Once each group has equilibrated to
$\nu_{s'}(\cdot \mid b_{s'})$, the remaining violation is purely
inter-group: $\Viol \approx \Viol_{\text{inter}}$. By
macro-sufficiency, $\Viol_{\text{inter}}$ under local equilibrium
depends on the micro-boundary only through the macro-boundary
$\tilde{b}_{s'}$, and hence through the macro-state
$\tilde{\omega} = g(\omega)$, so
$\mathbb{E}[\Viol_{\text{inter}} \mid \text{local eq.}]
= \widetilde{\Viol}(\tilde{\omega})$.

By the inter-group contraction assumption~\ref{cond:inter-contraction},
each inter-group update reduces the expected macro-violation:
\begin{align*}
  \mathbb{E}[\widetilde{\Viol}(\tilde{\omega}_{t+1}) \mid \tilde{\omega}_t]
  &\leq (1-\alpha)\, \widetilde{\Viol}(\tilde{\omega}_t)
   + \varepsilon(\eta, \rho),
\end{align*}
where $\varepsilon(\eta, \rho) \to 0$ as
$\eta\tau_{\text{fast}} \to 0$ (\cref{lem:effective-markov}).  The
error term vanishes as the timescale separation strengthens, yielding
the macro-drift condition with $\alpha' > 0$ for sufficiently
strong separation.
\end{proof}

\begin{lemma}[Macro-Exploration Inherits $\varphi$-Irreducibility]%
\label{lem:macro-explore}
If the micro-kernel $K$ is $\varphi$-irreducible
(\cref{def:exploration}) with full exploration support, $g$ is
continuous and surjective, and the partition $\pi$ is finite, then
the effective macro kernel $\tilde{K}$ (\cref{lem:effective-markov})
is $\tilde{\varphi}$-irreducible on $\tilde{\Omega}$, where
$\tilde{\varphi} = g_\# \varphi$ is the pushforward of $\varphi$
under $g$.  Moreover, $\tilde{\varphi}$ has full exploration
support with respect to $\tilde{\mathcal{F}} = g(\mathcal{F})$.
\end{lemma}

\begin{proof}
Let $\tilde{A} \subseteq \tilde{\Omega}$ be measurable with
$\tilde{\varphi}(\tilde{A}) > 0$, and let
$\tilde{\omega} \in \tilde{\Omega}$.  Define
$A = g^{-1}(\tilde{A}) \subseteq \Omega$.  Since
$\varphi(A) = \tilde{\varphi}(\tilde{A}) > 0$ (by definition of
pushforward), $\varphi$-irreducibility of $K$ gives: for any
$\omega \in g^{-1}(\tilde{\omega})$,
$\sum_{n \geq 1} K^n(\omega, A) > 0$.  In particular, there exists
$n$ such that $K^n(\omega, A) > 0$, so the slow-time skeleton
chain (which includes the full-kernel transitions at inter-group
update times) also reaches $A$ with positive probability.  Since
$\tilde{X}_k = g(X_{t_k})$ and $A = g^{-1}(\tilde{A})$,
$\sum_{k \geq 1} \tilde{K}^k(\tilde{\omega}, \tilde{A}) > 0$.

For full exploration support: if $\tilde{U} \subseteq \tilde{\Omega}$
is open with $\tilde{U} \cap \tilde{\mathcal{F}} \neq \emptyset$,
then $g^{-1}(\tilde{U})$ is open in $\Omega$ (by continuity of $g$)
and meets $\mathcal{F}$ (since
$g(\mathcal{F}) = \tilde{\mathcal{F}}$), so
$\varphi(g^{-1}(\tilde{U})) > 0$ by the micro-level full exploration
support, giving $\tilde{\varphi}(\tilde{U}) > 0$.
\end{proof}

\begin{lemma}[Macro-Coherence]\label{lem:macro-coherence}
If the unique stationary distribution $\tilde{\mu}^*$ of the
effective macro kernel $\tilde{K}$ (\cref{lem:effective-markov}) has
a non-product $(s'_1, s'_2)$-marginal for at least one macro-edge
$\{s'_1, s'_2\} \in E'$, then $\Coh(\tilde{\mu}^*) > 0$.
\end{lemma}

\begin{proof}
Direct application of \cref{lem:coherence} to $\tilde{\mu}^*$ on
$\tilde{\Omega}$: the non-product $(s'_1, s'_2)$-marginal
hypothesis gives
$D_{\mathrm{KL}}(\tilde{\mu}^*_{s'_1,s'_2} \| \tilde{\mu}^*_{s'_1}
\otimes \tilde{\mu}^*_{s'_2}) > 0$, which lower-bounds
$\Coh(\tilde{\mu}^*)$ by the data processing inequality.
(As at the micro level, the non-product condition must be verified
for the specific macro-dynamics; the heuristic of
\cref{rem:coherence-application} can guide this verification but
does not replace it.)
\end{proof}

\begin{theorem}[Closure Under Coarse-Graining (conditional)]\label{thm:closure}
Let $\mathcal{C}$ be a cognitive system, $\pi : S \twoheadrightarrow S'$
a partition map, and $\{g_{s'}\}$ compression maps with continuous,
surjective $g : \Omega \to \tilde{\Omega}$. If:
\begin{enumerate}[nosep]
  \item[(i)] The micro-dynamics exhibits $\eta$-timescale separation with
    respect to $\pi$ and $\{g_{s'}\}$ (\cref{def:timescale}), with
    uniform geometric mixing~\ref{cond:uniform-mixing} and lifting
    kernel regularity~\ref{cond:lifting-regularity}.
  \item[(ii)] The macro-constraint graph $G'$ is connected.
  \item[(iii)] The macro-feasible set
    $\tilde{\mathcal{F}}_{\min} :=
    \arg\min \widetilde{\Viol}$ is non-empty and compact (automatic
    on compact $\tilde{\Omega}$ with continuous
    $\widetilde{\Viol}$), the inter-group resolution satisfies the
    contraction condition~\ref{cond:inter-contraction}, and
    $\tilde{\mu}^*$ has a non-product marginal along at least one
    macro-edge (see \cref{lem:macro-coherence}).
  \item[(iv)] \textbf{Macro-regularity.}  The effective macro kernel
    $\tilde{K}$ (\cref{eq:macro-kernel}) is weak Feller on compact
    $\tilde{\Omega}$, and admits a minorization
    $\tilde{K}(\tilde{\omega}, \tilde{A}) \geq
    \tilde{\delta}\,\tilde{\nu}(\tilde{A})$ for $\tilde{\omega}$ in
    a compact sublevel set
    $\tilde{C} = \{\tilde{\omega} : \widetilde{\Viol}(\tilde{\omega})
    \leq c\}$.
\end{enumerate}
Then $\tilde{\mathcal{C}} = \Gamma_{\pi,g}(\mathcal{C})$ is a cognitive
system on the compressed macro-state space $\tilde{\Omega}$.
\end{theorem}

\begin{proof}[Proof (structural argument)]
A fully rigorous proof requires discrete-time averaging estimates
for the timescale-separation limit; we provide the structural
argument, deferring analytic details to the individual lemmas.
We verify each component of \cref{def:cognitive-system} for
$\tilde{\mathcal{C}}$:

\textbf{Constraint network.}
The macro-network
$\tilde{\mathcal{N}} = (S', \tilde{\mathcal{D}}, G', \tilde{\mathcal{R}})$
is well-defined by the construction in \cref{def:coarse-graining}. The
macro-state spaces $\tilde{D}_{s'}$ are images of compact sets under
continuous maps (hence compact). The macro-constraints
$\tilde{\mathcal{R}}$ are images of micro-constraints under the
compression (hence universally measurable, as in
\cref{def:coarse-graining}).

\textbf{Effective macro kernel.}
By \cref{lem:effective-markov}, the slow-time macro-process
$\{\tilde{X}_k\}$ is (approximately) a time-homogeneous Markov chain on
$\tilde{\Omega}$ with effective kernel $\tilde{K}$.  The regularity
properties required for convergence---weak Feller property and
minorization---are assumed directly in condition~(iv).  (In
specific applications, these can often be verified from the
structure of $K_{\text{slow}}$ and $\rho_{\tilde{\omega}}$; we
state them as explicit assumptions to avoid claiming inheritance
from the micro-chain, which would require stronger lumpability
conditions.)

\textbf{Exploration.}
By \cref{lem:macro-explore}, the effective macro kernel $\tilde{K}$
is $\tilde{\varphi}$-irreducible on $\tilde{\Omega}$ with
$\tilde{\varphi} = g_\# \varphi$ having full exploration support
with respect to $\tilde{\mathcal{F}}$.

\textbf{Resolution.}
By \cref{lem:macro-drift}, the effective macro kernel $\tilde{K}$
satisfies a Foster--Lyapunov drift condition on $\tilde{\Omega}$
for the Lyapunov function
$\tilde{L} = \widetilde{\Viol} + 1$, and the
updates are local with respect to $G'$ (they depend only on the
macro-states of neighboring macro-components, by macro-sufficiency).

\textbf{Goal-stabilizing pattern.}
Applying \cref{thm:convergence} to the macro-system on $\tilde{\Omega}$
(whose conditions are verified above): the macro-dynamics converges to a
unique stationary distribution $\tilde{\mu}^*$ that concentrates near
$\tilde{\mathcal{F}}_{\min}$ in the sense of
\cref{eq:markov-concentration} (by the macro-drift condition and
compactness; as with the micro-level result, the chain is ergodic with
full support, so ``attraction'' is stochastic).
By condition~(iii), $\tilde{\mu}^*$ has a non-product marginal along at
least one macro-edge, so $\Coh(\tilde{\mu}^*) > 0$ by
\cref{lem:macro-coherence}. The Lyapunov function is
$\tilde{L}(\tilde{\omega}) = \widetilde{\Viol}(\tilde{\omega}) + 1$.

Hence all four components of a cognitive system are present, and
$\tilde{\mathcal{C}}$ is cognitive.
\end{proof}

\begin{remark}[When the closure assumptions hold]\label{rem:closure-scope}
The closure theorem is conditional: DCR is closed under
coarse-graining \emph{when coarse-graining yields an effective
Markovian theory}.  Two concrete classes where the assumptions are
standard:
\begin{enumerate}[nosep]
  \item \emph{Finite-state chains with exact lumpability.}  If the
    partition $\pi$ is a \emph{lumpable} partition of a finite
    Markov chain (in the sense of Kemeny--Snell), the macro-chain
    is exactly Markov, all regularity conditions hold trivially,
    and the closure is exact.
  \item \emph{Metastable Markov state models.}  In molecular
    dynamics and related fields, spectral gap analysis identifies
    metastable macro-states with slow inter-group transitions and
    fast intra-group mixing---precisely the timescale separation
    of \cref{def:timescale}.  The resulting Markov state models
    satisfy the closure assumptions approximately, with the error
    controlled by the spectral gap ratio.
\end{enumerate}
The conceptual point is that the DCR triad is preserved whenever
the physics admits a meaningful macro-description: the assumptions
isolate exactly the conditions under which ``zooming out'' yields
a new DCR-system rather than a non-Markovian mess.  When those
conditions fail (overlapping timescales, strong memory effects),
DCR still applies at each scale individually---it is the
\emph{composition} across scales that requires the separation.
\end{remark}

\subsection{Dissolution of the Combination Problem}\label{ssec:combination}

The combination problem in panpsychism asks: if fundamental entities have
experience, how do micro-experiences combine into the unified macro-experience
of, say, a human mind? DCR addresses this problem constructively by replacing the notion of
``combining experiences'' with a formal construction:

\begin{corollary}[Combination via Coarse-Graining]\label{cor:combination}
Let $\mathcal{C}$ be a cognitive system at scale $n$ with partition $\pi$
and compression $\{g_{s'}\}$ satisfying the conditions of
\cref{thm:closure}. Then:
\begin{enumerate}[nosep]
  \item The intra-group cognitive dynamics at scale $n$ (exploration and
        resolution within each $\pi^{-1}(s')$) constitutes the
        \emph{exploration component} of the scale-$(n+1)$ system---the
        residual fluctuations of equilibrated groups, projected through
        $g_{s'}$ onto the macro-state space, provide the stochasticity
        that drives macro-exploration (\cref{lem:macro-explore}).
  \item The inter-group constraint resolution at scale $n$ constitutes the
        \emph{resolution component} of the scale-$(n+1)$ system---the
        reduction of macro-violation (\cref{lem:macro-drift}).
  \item The macro-attractor $\tilde{\mathcal{A}} = g(\mathcal{A})$ is the
        coherent pattern into which the micro-cognitive processes compose
        (\cref{lem:macro-coherence}).
\end{enumerate}
\end{corollary}

\begin{proof}
Each claim follows directly from the corresponding lemma used in the proof
of \cref{thm:closure}. The key observation is structural: the compression
maps $\{g_{s'}\}$ discard the fast, intra-group micro-detail (which has
already been resolved at scale $n$) and retain only the slow
macro-observables. The coarse-graining construction maps the resolution of
scale $n$ to the exploration of scale $n+1$ because once intra-group
constraints are resolved (fast timescale), the residual fluctuations
\emph{in the compressed representation} are precisely what the macro-level
``explores.'' The construction does not merely aggregate states---it
\emph{compresses and transmutes} one level's resolution into the next
level's exploration, providing a constructive mechanism for cross-scale
composition that genuinely reduces degrees of freedom.
\end{proof}

There is no mystery of ``combination'' because there is no separate substance
(experience, qualia) that needs combining. There is only the DCR process,
recurring at each scale via coarse-graining. The compression maps formalize
how micro-detail is discarded at each level: a neuron's molecular state is
compressed to its firing rate; a population's firing rates are compressed to
a mean-field activation; a cortical column's activations are compressed to a
feature representation. What we call ``unified experience'' at the human
scale is the coherent attractor of a cognitive system whose components are
themselves compressed cognitive systems, recursively.

The \emph{micro-choices} framing (\cref{rem:micro-choices}) sharpens this
point.  What is fundamental at each scale is not ``micro-experience'' or
``micro-qualia'' but \emph{micro-degrees of freedom}---hidden variables,
internal microstates, symbol orderings---whose structured resolution under
constraints produces the exploration noise that the next scale up
inherits.  Coarse-graining turns micro-choice variability into
macro-level exploration (\cref{cor:combination}, item~1): the residual
fluctuations of equilibrated groups, projected through the compression
maps, are precisely the stochastic input that drives macro-level
constraint resolution.  This avoids the ``combination of qualia'' trap
entirely: there are no qualia to combine, only degrees of freedom to
compress.

\subsection{The Depth of Cognition}\label{ssec:depth}

Not all cognitive systems are equally ``intelligent.'' We introduce a measure
of cognitive depth:

\begin{definition}[Cognitive Depth]\label{def:depth}
The \emph{cognitive depth} of a system is the number of nested
coarse-graining levels $k$ at which the DCR triad is simultaneously active:
\begin{equation}\label{eq:depth}
  \delta(\mathcal{C}) = \max\{k \mid \Gamma_{\pi_k,g_k} \circ \cdots \circ
  \Gamma_{\pi_1,g_1}(\mathcal{C}) \text{ is cognitive}\},
\end{equation}
where the maximum is over all hierarchical sequences of partitions
$(\pi_1, \ldots, \pi_k)$ and compression maps $(g_1, \ldots, g_k)$
satisfying the conditions of \cref{thm:closure} at each level, subject
to the \emph{strict reduction} requirement $|S_{i+1}| < |S_i|$ at each
level~$i$ (i.e., the partition $\pi_i$ is non-trivial: at least one
group contains more than one component).  Since $|S|$ is finite, this
ensures $\delta(\mathcal{C}) \leq |S| - 1 < \infty$.
\end{definition}

To illustrate the scale: a hydrogen atom has depth~1 (quantum constraint
resolution at the particle level); a bacterium has depth $\sim$3--4
(molecular, metabolic, behavioral); a human brain has depth $\sim$6--8
(ionic, synaptic, columnar, areal, network, behavioral, social).  These
estimates are based on empirically identifiable organizational levels and
should be read as illustrative ordinal rankings, not precise measurements
(see \cref{rem:depth-computability}).  The definition provides a
qualitative, non-anthropocentric ordering of intelligence without
requiring a binary threshold.

\begin{remark}[Computability of depth]\label{rem:depth-computability}
Computing $\delta(\mathcal{C})$ exactly requires optimizing over all
hierarchical partition sequences $(\pi_1, \ldots, \pi_k)$ and compression
maps $(g_1, \ldots, g_k)$, verifying the DCR conditions at each
level---a combinatorially intractable problem in general. The numerical estimates above are based on \emph{empirically
observable} organizational levels (e.g., the well-established hierarchy from
ion channels to brain areas) rather than exhaustive search. In practice,
$\delta$ serves as a coarse ordinal ranking rather than a precise cardinal
measure: distinguishing depth~2 from depth~6 is meaningful; distinguishing
depth~6 from depth~7 requires detailed empirical verification of the DCR
triad at each level.
\end{remark}

% ==========================================================================
\section{Physical Instantiations of DCR}\label{sec:physics}
% ==========================================================================

We now exhibit structural witnesses that fundamental physical processes
instantiate the DCR triad, supporting the claim that the cosmos is
cognitive at every scale.  For each example, we identify the five DCR
ingredients using the following checklist:

\begin{enumerate}[nosep,label=(\roman*)]
  \item \emph{Components \& degrees of freedom} --- what are the
    interacting parts and their local state spaces?
  \item \emph{Constraints} --- what local compatibility conditions
    (conservation laws, fitness, synaptic weights) couple neighboring
    components?
  \item \emph{Exploration} --- what mechanism generates variability
    across the configuration space (quantum fluctuations, mutation,
    stochastic firing)?
  \item \emph{Resolution} --- how do local interactions reduce
    constraint violation without global coordination?
  \item \emph{Stable coherent attractor} --- what is the coherent
    macroscopic pattern that emerges (completed transaction, convection
    roll, adapted species, neural representation)?
\end{enumerate}

\noindent
\Cref{ex:benard} carries out a detailed verification sketch
(conditions \ref{cond:drift}--\ref{cond:full-support}) for
thermodynamic self-organization (see \cref{rem:benard-caveat} for
caveats on the proof-sketch character); the remaining examples are
presented at the level of structural mappings, with a remark
(\cref{ex:benard}, following) on how the same verification
template applies.

\subsection{Quantum Mechanics}\label{ssec:quantum}

Consider a system of $N$ interacting quantum particles (or field modes).
The standard decoherence account \citep{zurek2003decoherence} treats
the suppression of off-diagonal coherences as the mechanism by which
classical reality emerges, but decoherence alone never yields a definite
outcome---it produces an improper mixture, not a selected event. The DCR
structure of quantum mechanics is most transparently exhibited by the
\emph{Transactional Interpretation} (TIQM)
\citep{cramer1986transactional}, which builds on the Wheeler--Feynman
absorber theory \citep{wheeler1945interaction}. In TIQM, every quantum
event is a \emph{transaction}---a completed handshake between emitter and
absorber mediated by retarded (offer) and advanced (confirmation) waves.
We use TIQM as a \emph{structural witness} that quantum dynamics can be
read as a distributed constraint-resolution process; DCR does not require
endorsing any particular interpretation of quantum mechanics.  The mapping
onto the DCR triad is as follows:

\begin{itemize}[nosep]
  \item \textbf{Components:} Emitter and absorber sites---the vertices of
        the spacetime interaction graph.
  \item \textbf{Degrees of freedom:} The possible quantum states (energy,
        momentum, polarization, spin) at each site, drawn from the local
        Hilbert space $\mathcal{H}_s$.
  \item \textbf{Exploration:} The \emph{offer wave} $\psi$ (retarded wave)
        propagates from the emitter to all potential absorbers, exploring
        every possible transaction partner simultaneously. In the Feynman
        path integral formulation \citep{feynman1948space}, this is the sum
        over all paths weighted by $e^{iS/\hbar}$---the emitter explores the
        entire accessible configuration space.
  \item \textbf{Constraints:} Conservation laws (energy, momentum, angular
        momentum, charge) at each interaction vertex. For an edge
        $\{s,s'\}$ in the interaction graph, the constraint $R_{\{s,s'\}}$
        requires that the quantum numbers carried by the offer wave from
        emitter $s$ match those that absorber $s'$ can accept, given $s'$'s
        own state and the applicable conservation laws.
  \item \textbf{Resolution:} The \emph{confirmation wave} $\psi^*$ (advanced
        wave) propagates from each potential absorber back to the emitter.
        The Wheeler--Feynman handshake is distributed constraint resolution:
        each absorber independently evaluates the offer against its local
        constraints and responds; the transaction that forms is the one
        where all constraints are simultaneously satisfied across both
        endpoints. There is no central selector choosing the outcome---the
        definite result emerges from the mutual satisfaction of local
        conservation laws, distributed across spacetime.
  \item \textbf{Stable pattern:} The \emph{completed transaction}---a
        definite, irreversible transfer of conserved quantities between
        emitter and absorber. Once formed, the transaction is a classical
        fact: the stable coherent attractor of the handshake process.
\end{itemize}

The retrocausal structure is not a defect but a feature: constraints
propagate both forward and backward in time, making the resolution
genuinely distributed across spacetime rather than confined to a single
time-slice. What is conventionally called ``wavefunction collapse'' is,
in DCR terms, the convergence of a distributed constraint satisfaction
process to its feasible solution.

For $N$ interacting particles, the interaction graph $G$ has particles as
vertices and pairwise interactions as edges. Each interaction is a
potential transaction site where conservation constraints must be locally
satisfied. The global physical outcome---the set of completed
transactions---is the configuration $\omega \in \mathcal{F}$ where all
local constraints are simultaneously met: the feasible set of the
constraint network, reached by distributed resolution without global
coordination.

\begin{remark}[Selection and collapse]
The structural identity between quantum collapse and natural selection
(\cref{ssec:biology}) is not accidental. In both cases, multiple
possibilities are explored (superposition / genetic variation),
constraints select which possibilities are realized (conservation laws /
fitness landscape), and the selection is local and distributed (each
absorber / each organism evaluates constraints independently). DCR
identifies both as instances of the same universal process, differing
only in the physical substrate and timescale.
\end{remark}

\begin{remark}[Interpretation-independence]\label{rem:interpretation-independence}
The DCR mapping does not depend on retrocausality.  One can equally
express the same constraint-resolution structure using path-integral
or decoherence-based language, where ``resolution'' corresponds to
consistency of histories and environment-induced selection.  TIQM is
used only because the constraint-satisfaction structure is explicit
in that formulation.
\end{remark}

\begin{remark}[Discrete ontic structure and micro-choices]\label{rem:micro-choices}
The DCR picture of quantum mechanics does not require the continuum
structure of canonical quantum theory.
\citet{powers2024statistical} model measurement events as event
networks whose nodes and edges are built from finite base-2 (and
base-4/base-16) symbol sequences with XOR-like (addition modulo two)
composition.  Observable quantum numbers correspond to symbol
\emph{counts}---coarse summaries of the underlying sequences---while
the ordering of symbols within each sequence remains hidden.  This
hidden ordering is the source of non-determinism (in their phrase:
``counts are generally observable, but sequences are not'')
\citep{powers2024statistical}.

In DCR terms, the space of admissible ontic configurations (all
symbol orderings consistent with the quantum numbers) is the
\emph{exploration space}; the contextual compatibility
conditions---which orderings are consistent with both observers'
measurement events---are the \emph{constraints}; and the observed
probability distribution, given by relative frequencies of surviving
configurations, is the \emph{stabilized pattern}.  The individual
symbol orderings are \emph{micro-choices}: local, hidden degrees of
freedom whose structured resolution produces the macroscopic outcome.

For finite sequence length~$n$, the model predicts small but
unavoidable deviations from canonical QM due to discrete granularity
(e.g., rotation angles are effectively rational-valued), with
improved agreement as~$n$ increases.  \citet{powers2024statistical}
propose optical tabletop tests to constrain these $n$-dependent
deviations.  Since all empirical measurement data is inherently
discrete, the finite-$n$ model is not merely an approximation to
the continuous theory but a plausible candidate for a more
fundamental description.  If this view is correct, it supports the
DCR framework's assumption of finite discrete components
(\cref{rem:finiteness}): the continuum would be an idealization of
an underlying discrete constraint resolution process operating
through micro-choices.
\end{remark}

\subsection{Thermodynamic Self-Organization}\label{ssec:thermo}

We model Rayleigh--B{\'e}nard convection as a DCR-system under a
standard lattice discretization, illustrating how the DCR axioms
map onto a concrete physical system.  The verification is a
sketch, not a full proof (see \cref{rem:benard-caveat}).

Consider a fluid layer between horizontal plates, heated from below
($T_H$) and cooled from above ($T_C$), in the Boussinesq approximation
\citep{chandrasekhar1961hydrodynamic}. We discretize the fluid domain on a
regular $d$-dimensional lattice with $N$ parcels and spacing $h$.

\begin{example}[B{\'e}nard Convection as a Cognitive System]%
\label{ex:benard}
We claim that for Rayleigh number
$\mathrm{Ra} > \mathrm{Ra}_c$ (above the convective instability
threshold), the stochastic lattice Boussinesq system with thermal
fluctuations constitutes a cognitive system
$\mathcal{C} = (\mathcal{N}, \{X_t\}, R, \mathcal{A})$ in the
sense of \cref{def:cognitive-system}, under the following standard
assumptions: (i)~the finite-difference discretization preserves the
energy dissipation structure of the continuous Boussinesq equations,
(ii)~the spectral gap $\lambda_1$ of the graph Laplacian on the
lattice is positive, and (iii)~the Gaussian noise amplitude
$\sigma > 0$ is fixed.
\end{example}

\begin{proof}[Verification sketch]
We construct the DCR tuple and verify conditions
\ref{cond:drift}--\ref{cond:full-support} of \cref{def:combined};
see \cref{rem:benard-caveat} for caveats on the level of rigor.

\textbf{Constraint network.}
Let $S = \{1,\ldots,N\}$ index the parcels, with state space
$D_s = [-v_{\max}, v_{\max}]^d \times [T_C, T_H]$ for each $s$
(velocity and temperature, bounded by the energy balance). The
configuration space $\Omega = \prod_s D_s$ is compact. The interaction
graph $G$ is lattice adjacency (connected for $N \geq 2$). For each edge
$e = \{s,s'\} \in E$, the constraint relation $R_e$ encodes the
discretized Boussinesq conservation laws at the interface:
$(\omega_s, \omega_{s'}) \in R_e$ iff the discrete incompressibility
condition $\nabla^h_{ss'} \cdot \mathbf{v} = 0$, the discrete momentum
balance
$\nu\,\nabla^{h,2}_{ss'}\mathbf{v}
 - (\mathbf{v}\cdot\nabla^h_{ss'})\mathbf{v}
 + \beta(T-T_{\mathrm{ref}})\mathbf{g}
 = \nabla^h_{ss'} p$,
and the discrete energy balance
$\kappa\,\nabla^{h,2}_{ss'}T
 = (\mathbf{v}\cdot\nabla^h_{ss'})T$
are satisfied at the $\{s,s'\}$ interface, where $\nabla^h_{ss'}$ and
$\nabla^{h,2}_{ss'}$ denote the standard finite-difference gradient and
Laplacian operators, $\nu$ is kinematic viscosity, $\kappa$ is thermal
diffusivity, $\beta$ is the thermal expansion coefficient, and
$\mathbf{g}$ is gravitational acceleration.

\textbf{Violation.}
The pairwise violation $v_e(\omega_s, \omega_{s'})$ is the sum of squared
residuals of the three conservation equations at the $\{s,s'\}$ interface.
Then $v_e \geq 0$ with equality iff the steady-state Boussinesq equations
are locally satisfied, and $\Viol$ is continuous on compact $\Omega$.

\textbf{Dynamics.}
The resolution kernel $R$ is a semi-implicit Gauss--Seidel sweep of the
discretized Boussinesq equations: each parcel updates its velocity and
temperature using only the current states of its lattice neighbors,
satisfying the locality requirement of \cref{def:resolution}. The
exploration kernel $E$ adds Gaussian noise of variance
$\sigma^2 \propto k_B T_{\mathrm{ref}}/(\rho\, h^d)$ (fluctuating
hydrodynamics), projected onto $\Omega$:
\[
  E(\omega, \cdot) = \mathrm{Law}\!\bigl(
    \Pi_\Omega(\omega + \sigma\,\boldsymbol{\xi})\bigr),
  \qquad \boldsymbol{\xi} \sim \mathcal{N}(0, I_{N(d+1)}).
\]

\textbf{Verification of conditions.}
\begin{enumerate}[nosep,label=(\alph*)]
  \item \emph{Foster--Lyapunov drift.} The energy functional
    $\mathcal{E}(\omega) = \sum_s\bigl[\tfrac{1}{2}|\mathbf{v}_s|^2
    + \tfrac{1}{2}|T_s - T_{\mathrm{lin},s}|^2\bigr]$
    (where $T_{\mathrm{lin}}$ is the linear conduction profile) satisfies
    \[
      \mathcal{E}(\omega^{(t+1)}) \leq \mathcal{E}(\omega^{(t)})
      - \Delta t\bigl[\nu\,\lVert\nabla^h\mathbf{v}\rVert^2
      + \kappa\,\lVert\nabla^h\theta\rVert^2\bigr]
      + \Delta t\,\mathrm{Ra}\,(\theta, v_z)
    \]
    under the resolution step, where $\theta = T - T_{\mathrm{lin}}$.
    By the discrete Poincar{\'e} inequality
    ($\lVert\nabla^h u\rVert^2 \geq \lambda_1\lVert u\rVert^2$ with
    $\lambda_1 > 0$ the spectral gap of the graph Laplacian), the
    dissipative terms dominate the buoyancy source outside a compact
    neighborhood $C$ of the convection roll attractor, yielding a
    Foster--Lyapunov drift condition
    (\cref{rem:drift-sufficient}) for $L = \Viol + 1$ with small
    set $C$.  (The transfer from $\mathcal{E}$ to $\Viol$ near the
    attractor follows from the comparability of the respective
    Hessians under the linearized Boussinesq operator---a standard
    result in the stability theory of discretized
    Navier--Stokes systems.)
  \item \emph{$\varphi$-irreducibility.} The Gaussian kernel $E$ has
    positive density on $\mathrm{int}(\Omega)$, so
    $K(\omega,A) \geq \epsilon\,E(\omega,A) > 0$ for any $A$ with
    positive Lebesgue measure $\lambda(A) > 0$. The chain is
    $\lambda$-irreducible.
  \item \emph{Minorization.} Choose $c$ large enough that
    the sublevel set
    $C = \{\omega \in \mathrm{int}(\Omega) : \Viol(\omega) \leq c\}$
    is visited infinitely often under the drift
    (condition~\ref{cond:drift}).  Since $C$ is compactly contained
    in the interior of $\Omega$, the (unprojected) Gaussian
    exploration density is bounded below by some $\delta > 0$ on
    $C \times C$.  Hence
    $K(\omega, A) \geq \epsilon\,\delta\,\lambda(A \cap C)$ for
    $\omega \in C$, establishing the minorization
    condition~\ref{cond:minorization} with
    $\nu = \lambda(\cdot \cap C)/\lambda(C)$.  (The boundary of
    $\Omega$, where the projection $\Pi_\Omega$ distorts densities,
    is avoided by taking $C \subset \mathrm{int}(\Omega)$; see
    \cref{rem:benard-caveat}.)
  \item \emph{Weak Feller.} The Boussinesq update is polynomial in
    $\omega$ and neighbor states (hence continuous); the Gaussian density
    is smooth. Their convex combination $K$ is weak Feller.
  \item \emph{Full exploration support.} The Lebesgue-equivalent
    irreducibility measure charges every open set, in particular those
    meeting $\mathcal{F}$.
\end{enumerate}

\textbf{Attractor and coherence.}
For $\mathrm{Ra} > \mathrm{Ra}_c$, the feasible set $\mathcal{F}$
(steady-state solutions of the discretized Boussinesq equations) includes
the convection roll patterns \citep{cross1993pattern}. The
Gauss--Seidel update of parcel~$s$ makes
$(\mathbf{v}_s^{(t+1)}, T_s^{(t+1)})$ depend on the states of
neighbors~$s'$ through the discretized momentum and energy equations,
so the unique $\mu^*$ has non-product $(s,s')$-marginals
(\cref{rem:coherence-application}).
By \cref{thm:convergence}, the chain converges to a unique
$\mu^*$ with $\Coh(\mu^*) > 0$. The Lyapunov function is $L = \Viol + 1$, and the
convection rolls constitute the stable coherent attractor $\mathcal{A}$.
\end{proof}

The convection pattern is \emph{not} at equilibrium (DCR correctly
excludes equilibrium), and it arises without any central controller
selecting the pattern. It is a cognitive system of depth~1.

\begin{remark}[Proof-sketch character of the B{\'e}nard verification]%
\label{rem:benard-caveat}
Three aspects of the above verification merit further care in a
fully rigorous treatment.  (i)~The constraints encode the
\emph{steady-state} Boussinesq equations, while the dynamics is
\emph{time-evolution}; the violation functional therefore measures
departure from stationarity, and the drift argument implicitly
conflates energy dissipation (a property of the PDE) with violation
reduction (a property of the discretized update rule).  The
transfer from the energy functional $\mathcal{E}$ to
$\Viol$ near the attractor requires comparability of their
respective Hessians under the linearized Boussinesq operator---a
standard result in discretized Navier--Stokes stability theory, but
one whose constants depend on the lattice spacing~$h$.
(ii)~The projection $\Pi_\Omega$ onto the compact box breaks the
smoothness of the Gaussian exploration kernel at the boundary
of~$\Omega$; hence the ``continuous and positive'' density claim in
the minorization step holds only on the interior, and a boundary
layer analysis is needed to establish the minorization uniformly
on the sublevel set~$C$.
(iii)~The spectral gap $\lambda_1$ of the graph Laplacian depends on
the lattice discretization, and the drift constants
$\lambda, b$ depend on $h$ and $N$; the verification is for a fixed
discretization and does not address the continuum limit $h \to 0$.
These issues are standard in the numerical analysis of stochastic
PDE discretizations; we highlight them to be explicit about the
level of rigor.
\end{remark}

\begin{remark}[Formalizability of other physical examples]
The quantum (\cref{ssec:quantum}), biological (\cref{ssec:biology}), and
neural (\cref{ssec:neural}) examples admit analogous formalizations. In
each case, the construction follows the same template as
\cref{ex:benard}: identify components and pairwise constraints, define
the violation via squared residuals of the governing equations, specify
the resolution and exploration kernels, and verify (or sketch
verification of) conditions \ref{cond:drift}--\ref{cond:full-support}.
We present these examples informally below, noting that the exploration
mechanism (quantum
fluctuations, genetic mutation, stochastic neural firing) and constraint
structure (interaction Hamiltonians, fitness landscapes, synaptic weights)
change while the verification structure remains the same.
\end{remark}

\begin{remark}[Related formal program: neural-network universe]%
\label{rem:nn-universe}
\citet{vanchurin2020world} proposes that the universe at its most
fundamental level is a neural network with two tiers of dynamical
degrees of freedom: \emph{trainable variables} (weights, biases) and
\emph{hidden variables} (neuron states).  He shows that near
equilibrium, the trainable-variable dynamics is well approximated by
Madelung/Schr\"odinger-type equations (with free energy playing the
role of the phase), while further from equilibrium the same dynamics
yields Hamilton--Jacobi behavior.  In a coarse-grained limit, the
hidden-variable dynamics can produce emergent relativistic strings and,
via an Onsager-symmetry argument for entropy production, an
Einstein--Hilbert-like gravitational term.  This is a concrete instance
of distributed constraint resolution producing stable macroscopic
laws: the network's learning dynamics explores weight space (trainable
variables) and hidden-state space under local update rules, resolves
constraints imposed by the loss landscape and inter-neuron
interactions, and stabilizes into effective equations exhibiting
Schr\"odinger/Madelung-like and Einstein--Hilbert-like forms under
specific assumptions and limits.

We treat such ``neural-network universe'' models as
substrate-specific realizations of DCR rather than competitors: they
propose one particular microphysics---a learning network---whose
effective behavior instantiates the explore--resolve--stabilize triad.
DCR is substrate-agnostic and makes no commitment to the universe
being literally a neural network; it claims only that the
\emph{process} of distributed constraint resolution is universal.
Vanchurin's result strengthens this claim by providing an explicit
derivation program in which a specific adaptive substrate yields
effective forms resembling major physics formalisms.
\end{remark}

\subsection{Biological Adaptation}\label{ssec:biology}

\begin{itemize}[nosep]
  \item \textbf{Components:} Organisms in a population.
  \item \textbf{Degrees of freedom:} Genotype/phenotype space.
  \item \textbf{Exploration:} Mutation, recombination, developmental noise.
  \item \textbf{Constraints:} Environmental fitness landscape, inter-organism
        competition, predator--prey relations.
  \item \textbf{Resolution:} Natural selection propagates constraints locally
        (each organism's survival depends on its local fitness, not a global
        optimization). This is inherently distributed.
  \item \textbf{Stable pattern:} Adapted species occupying fitness peaks---the
        coherent attractor of the evolutionary dynamics
        \citep{kauffman1993origins}.
\end{itemize}

Biological evolution is a cognitive system of depth $\geq 2$: the organisms
themselves are cognitive systems (metabolic constraint resolution), and the
population-level dynamics is a second layer of cognition.

\subsection{Neural Cognition}\label{ssec:neural}

\begin{itemize}[nosep]
  \item \textbf{Components:} Neurons (or neural populations).
  \item \textbf{Degrees of freedom:} Firing rates, membrane potentials,
        synaptic states.
  \item \textbf{Exploration:} Spontaneous activity, noise, stochastic
        neurotransmitter release.
  \item \textbf{Constraints:} Synaptic weights, lateral inhibition, top-down
        priors encoded in connectivity.
  \item \textbf{Resolution:} Local integration-and-fire dynamics; each neuron
        resolves its inputs against its threshold. Constraint propagation is
        distributed across the network.
  \item \textbf{Stable pattern:} Perceptual representations, motor plans,
        decisions---coherent attractors of the neural dynamics
        \citep{seth2021being}.
\end{itemize}

Neural cognition achieves high depth because the components (neurons) are
themselves biochemical cognitive systems, embedded in circuits that form
cognitive systems, embedded in areas, and so on up to whole-brain dynamics.

% ==========================================================================
\section{Recovery of Existing Frameworks}\label{sec:recovery}
% ==========================================================================

\subsection{Free Energy Principle as a Special Case}\label{ssec:fep-recovery}

\begin{proposition}[FEP Recovery]\label{prop:fep}
The Free Energy Principle is a special case of DCR obtained when:
\begin{enumerate}[nosep]
  \item The constraint network is bipartite, partitioned into ``internal''
        and ``external'' components with a Markov blanket boundary.
  \item The constraints encode a generative model
        $p(\tilde{s}, \psi \mid m)$ relating external causes $\psi$ to
        sensory observations $\tilde{s}$.
  \item The coherence measure is replaced by the negative variational free
        energy: $\Coh \mapsto -F = \mathbb{E}_q[\ln p(\tilde{s}, \psi)] -
        \mathbb{E}_q[\ln q(\psi)]$.
  \item The resolution dynamics is gradient descent on $F$
        (recognition dynamics).
\end{enumerate}
Under these specializations, DCR's ``explore--resolve--stabilize'' reduces to
FEP's ``prediction error minimization via active inference.''
\end{proposition}

\begin{proof}
We construct an explicit embedding of the FEP formalism into DCR.

\textbf{Step 1: Constraint network.}
Let $S = S_\mu \cup S_b \cup S_\eta$ be the decomposition into internal
($\mu$), blanket ($b$), and external ($\eta$) states. The Markov blanket
condition means $E$ contains no edges between $S_\mu$ and $S_\eta$ directly;
all coupling is mediated through $S_b$. This is a constraint network with
$G$ having the bipartite-through-blanket structure.

Define the constraint relations on blanket--internal edges via the generative
model: for $s \in S_\mu$, $s' \in S_b$,
\[
  R_{\{s,s'\}} = \{(\mu_s, b_{s'}) :
  p(\tilde{s}_{s'} \mid \mu_s) > 0\},
\]
encoding which internal states are consistent with which sensory observations.

\textbf{Step 2: Violation as surprise.}
The constraint violation functions are:
\[
  v_{\{s,s'\}}(\mu_s, b_{s'}) = -\ln p(\tilde{s}_{s'} \mid \mu_s).
\]
The total violation is then
$\Viol(\omega) = -\sum_{\{s,s'\}} \ln p(\tilde{s}_{s'} \mid \mu_s)$.
Note that $\Viol = 0$ requires $p(\tilde{s}_{s'} \mid \mu_s) = 1$
for all blanket components, which is generically impossible in
non-degenerate probabilistic models; accordingly, we use the
soft-constraint reading (\cref{rem:soft-constraints}), where the
feasible set is $\mathcal{F}_{\min} = \arg\min \Viol$ (the
internal states that best predict the observations).

To connect this to the FEP's surprisal $-\ln p(\tilde{s} \mid \mu)$, we
assume the generative model factorizes over blanket components conditioned
on internal states:
$p(\tilde{s} \mid \mu) = \prod_{s'} p(\tilde{s}_{s'} \mid \mu_{s(s')})$,
where $s(s')$ denotes the internal component coupled to blanket component
$s'$. Under this \emph{conditional independence} assumption (standard in
mean-field formulations of FEP), the sum reduces to:
$\Viol(\omega) = -\ln \prod_{s'} p(\tilde{s}_{s'} \mid \mu_{s(s')})
= -\ln p(\tilde{s} \mid \mu)$---the surprisal.

The variational free energy $F = \mathbb{E}_q[-\ln p(\tilde{s}, \psi)] +
\mathbb{E}_q[\ln q(\psi)]$ satisfies $F \geq -\ln p(\tilde{s})$ (by the
non-negativity of KL divergence), so $F$ is an upper bound on the surprisal,
hence on $\Viol$.

\textbf{Step 3: Resolution as free energy minimization.}
The FEP's recognition dynamics---gradient descent on $F$ with respect to
internal parameters---is a local update rule: each internal state $\mu_s$
adjusts based on its blanket neighbors.  In the FEP specialization, the
natural Lyapunov function is $F$ itself (rather than $\Viol + 1$):
gradient descent on a smooth function bounded below yields
$\mathbb{E}[F_{t+1}] \leq F_t - \alpha \|\nabla F\|^2$ for suitable
step size.  Since $F$ is bounded below and its sublevel sets are
compact (under standard regularity of the generative model), $F$
satisfies the Foster--Lyapunov drift condition~\ref{cond:drift} for
the combined recognition-plus-exploration kernel.  (Note that
$\Viol \leq F$ does \emph{not} by itself imply that descent on $F$
reduces $\Viol$; rather, $F$ serves as a valid Lyapunov function in
its own right, and concentration near low-$F$ regions entails
concentration near low-$\Viol$ regions via the bound.)

\textbf{Step 4: Exploration as active inference.}
FEP's active inference includes epistemic actions---perturbations to blanket
states that sample the environment. These provide the exploration kernel $E$:
the system probes configurations that might reduce uncertainty, ensuring
accessibility of low-violation regions.

\textbf{Step 5: Attractor.}
The attracting set under FEP is the set of internal states where
$F$ is minimized, i.e., $q(\psi) \approx p(\psi \mid \tilde{s})$. This is
a feasible configuration (minimal violation) and is coherent since internal
states are statistically coupled through the shared generative model.

Hence the FEP system $(\{S_\mu, S_b, S_\eta\},$ bipartite $G$, recognition
dynamics, epistemic exploration$)$ is a DCR cognitive system under the
specializations stated.
\end{proof}

DCR is strictly more general than FEP in two ways: (1)~it does not require
a bipartite structure with a Markov blanket, and (2)~it does not require
the constraints to be expressible as a generative model. Physical constraint
resolution (e.g., decoherence, convection) need not involve
``inference'' in any Bayesian sense.

\subsection{Integrated Information Theory as a Special Case}\label{ssec:iit-recovery}

\begin{proposition}[IIT Recovery]\label{prop:iit}
The integrated information $\Phi$ of IIT~2.0
\citep{tononi2004information} is recoverable from DCR's coherence measure
under the following specializations:
\begin{enumerate}[nosep]
  \item The constraint network encodes the transition probability matrix
        (TPM) of IIT: for each edge $\{s, s'\}$, the constraint $R_{\{s,s'\}}$
        encodes which state transitions of $s$ are compatible with the current
        state of $s'$.
  \item The analysis is restricted to a single time step (the TPM acts once).
  \item Coherence is refined to its irreducible component via the minimum
        information partition (MIP).
\end{enumerate}
\end{proposition}

\begin{proof}
\textbf{Step 1: From DCR coherence to total correlation.}
DCR's coherence measure (\cref{def:coherence}) is the multi-information
$\Coh(\mu) = D_{\mathrm{KL}}(\mu \,\|\, \bigotimes_s \mu_s)$, which
quantifies the total statistical dependence among components.

\textbf{Step 2: From total correlation to integrated information.}
IIT~2.0 defines $\Phi$ using KL divergence as follows. For a system in
state $x$ with TPM $T$, let $\mu_x = p(X_{t+1} \mid X_t = x)$ be the
one-step conditional distribution over successor states. For a bipartition
$\pi$ that cuts $S$ into parts $A$ and $B$, define the partitioned
distribution $p_\pi(X_{t+1} \mid x) = p(X^A_{t+1} \mid x) \otimes
p(X^B_{t+1} \mid x)$, which severs all inter-part dependencies. Then:
\[
  \Phi(x) = \min_{\pi \in \mathcal{P}}
  D_{\mathrm{KL}}\!\bigl(\mu_x \;\|\; p_\pi(\cdot \mid x)\bigr),
\]
where $\mathcal{P}$ is the set of bipartitions. Since
$D_{\mathrm{KL}}(\mu_x \| \bigotimes_s (\mu_x)_s) = \Coh(\mu_x)$ and
$D_{\mathrm{KL}}(\mu_x \| p_\pi(\cdot \mid x))$ measures only the
inter-part dependence (the intra-part dependencies within $A$ and $B$
cancel), we obtain:
\[
  \Phi(x) = \min_{\pi \in \mathcal{P}} \bigl[
    \Coh(\mu_x) - \Coh_A(\mu_x) - \Coh_B(\mu_x)
  \bigr],
\]
where $\Coh_A, \Coh_B$ denote the multi-information within each part.
That is, $\Phi$ extracts the \emph{irreducible} component of DCR's
coherence---the inter-part dependence that survives every possible
bipartition.

\textbf{Step 3: Static vs.\ dynamic.}
IIT computes $\Phi$ at a single time step. DCR's coherence is defined on
the stationary distribution $\mu^*$, which integrates over the full
dynamical trajectory. The IIT measure is recovered by restricting $\mu$
to the conditional distribution at a single step.

Hence $\Phi$ is a refinement of $\Coh$: it subtracts the reducible
component. DCR's $\Coh > 0$ is necessary for $\Phi > 0$ (since $\Phi \leq
\Coh$), but $\Coh > 0$ does not imply $\Phi > 0$ (a system can have
statistical dependencies that are fully decomposable). In this sense, IIT
imposes a \emph{stricter} coherence criterion than DCR's default measure.
\end{proof}

\begin{remark}[IIT versions]
The recovery above targets IIT~2.0, which uses KL divergence as its
distance measure. IIT~3.0 \citep{tononi2016integrated} replaces KL
divergence with the earth mover's distance (Wasserstein metric) and
defines $\Phi$ over cause--effect structures rather than single
distributions. The structural relationship---$\Phi$ as irreducible
coherence---still holds conceptually, but the formal identity with
DCR's $\Coh$ requires additional metric-space machinery that we do not
develop here. IIT~4.0 further introduces dynamical aspects that bring it
closer to DCR's process-level account, suggesting deeper convergence
between the frameworks at the level of recent formulations.
\end{remark}

DCR extends IIT in two directions: (1)~it provides a \emph{process-level}
account of how coherence arises through exploration and resolution, rather
than merely measuring it at a single time step; and (2)~it defines cognition
for systems where $\Phi$ is intractable (the computation is
$O(2^n)$) but the DCR triad---exploration, resolution, convergence to
coherent attractors---is empirically observable.

% ==========================================================================
\section{Predictions and Falsifiability}\label{sec:predictions}
% ==========================================================================

A framework that explains everything predicts nothing. DCR makes the following
falsifiable claims:

\begin{enumerate}
  \item \textbf{Coherence--exploration tradeoff.} In any cognitive system,
        there exists an optimal regime where exploration rate and constraint
        strength are balanced. Too much exploration (relative to constraint
        strength) yields incoherent dynamics; too much constraint yields
        rigid, brittle systems that fail to adapt. This predicts a universal
        inverted-U relationship between exploration rate and cognitive
        performance, testable in neural systems (cf.\ stochastic
        resonance \citep{gammaitoni1998stochastic}), evolutionary
        simulations, and optimization algorithms.

  \item \textbf{Depth predicts adaptability.} Systems with greater cognitive
        depth $\delta$ (\cref{def:depth}) should exhibit greater adaptability
        to novel environments, because deeper nesting provides more levels at
        which exploration--resolution can occur. This is testable: compare the
        adaptability of systems with different organizational depths (e.g.,
        single-celled vs.\ multicellular organisms, shallow vs.\ deep neural
        networks, flat vs.\ hierarchical organizations).

  \item \textbf{Critical constraint density.} There exists a critical density
        of constraints $|E|/|S|$ below which the system cannot sustain coherent
        attractors and above which the system becomes rigid. This parallels the
        satisfiability phase transition in random constraint satisfaction
        problems \citep{mezard2002random}, where a sharp transition from
        under-constrained (many solutions, low coherence) to over-constrained
        (no solutions, frozen dynamics) occurs at a critical clause-to-variable
        ratio. DCR predicts that this transition coincides with maximal
        cognitive capacity: near the critical point, the system exhibits
        signatures of self-organized criticality \citep{bak1987self}---power-law
        distributions of attractor sizes, long-range correlations, and maximal
        susceptibility. The novel prediction beyond the SAT literature is that
        this critical regime should also maximize coherence $\Coh(\mu^*)$
        and support the deepest cognitive nesting $\delta$. This is testable
        in constraint satisfaction problems and neural network models.

  \item \textbf{Coarse-graining preserves cognitive signatures.} If DCR is
        correct, then empirically measured coherence, exploration rates, and
        convergence timescales should obey scaling laws across levels of
        description of the same system (e.g., single-neuron vs.\ population
        vs.\ whole-brain dynamics). Specifically, the ratio of exploration
        timescale to resolution timescale should be approximately preserved
        under coarse-graining.
\end{enumerate}

% ==========================================================================
\section{Discussion}\label{sec:discussion}
% ==========================================================================

\subsection{The Cosmos as Cognitive}

If DCR is correct, then cognition is not an emergent property of brains---it is
what physics \emph{does}. The offer wave explores all possible absorbers; the
Wheeler--Feynman handshake resolves conservation constraints; the completed
transaction stabilizes into a classical fact. Thermal fluctuations explore the
space of flow configurations; the Navier--Stokes equations resolve constraints
locally between neighboring parcels; convection rolls stabilize. Mutation
explores genotype space; natural selection resolves fitness constraints;
adapted species stabilize. The same formal process, recurring at every
scale, connected by the coarse-graining construction.

The TIQM framing (\cref{ssec:quantum}) reveals a further unity: what physics
calls ``collapse'' and what biology calls ``selection'' are not merely
analogous but structurally identical---both are instances of distributed
constraint resolution in which multiple possibilities are explored and
local constraints determine which are realized (see \cref{ssec:quantum},
Remark). The retrocausal structure of the transactional interpretation
suggests that constraint resolution need not respect the arrow of time;
it is a relation among boundary conditions, not a process confined to one
temporal direction.

The discrete ontic model of \citet{powers2024statistical} lends additional
support to this picture.  If quantum probabilities arise from counting
admissible configurations of binary sequences---\emph{micro-choices} at
the level of symbol orderings---then what physics calls a ``quantum state''
is already a coarse-grained summary of a discrete constraint resolution
process (\cref{rem:micro-choices}).  The continuum of Hilbert space is
recovered only in the limit $n \to \infty$; at every finite scale the
system is a finite constraint network undergoing exploration, resolution,
and stabilization.  This dissolves the objection that DCR's discrete
formalism cannot capture continuous physics: the continuity is emergent,
not fundamental.

This is not panpsychism in the traditional sense. We do not claim that an
emitter--absorber pair ``has experiences.'' We claim that the transaction
by which a photon is emitted and absorbed is \emph{the same kind of
process} as the one by which a neuron participates in perception---formally,
structurally the same, as verified through the explicit construction in
\cref{ex:benard} and the structural mappings of
\cref{ssec:quantum,ssec:biology,ssec:neural}. ``Cognition'' is the name we
give to this process. Whether one wishes to call this ``experience'' at the
quantum level is a separate philosophical question that DCR does not
adjudicate.

\subsection{Relationship to Process Philosophy}

DCR exhibits structural resonances---not evidential dependencies---with
Whitehead's process philosophy \citep{whitehead1929process}, which held
that reality consists not of substances but of ``actual occasions'':
events of experience that ``prehend'' (take account of) their environment
and ``concresce'' into definite outcomes.  The correspondence is
suggestive: prehension maps onto exploration of degrees of freedom under
constraints imposed by neighboring occasions, and concrescence maps onto
resolution into a coherent, stabilized pattern.  We note these parallels
as interpretive context, not as independent support for DCR's formal
claims.

The TIQM framing sharpens the correspondence.  A Wheeler--Feynman
transaction---a discrete event in which multiple possibilities are
explored (the offer wave), constraints are propagated bidirectionally
(the confirmation wave), and a definite outcome concresces (the completed
transaction)---mirrors Whitehead's insistence that actual occasions are
constituted by their relations to both past and future, not built up
sequentially.  Whether this structural parallel reflects a deeper
ontological identity or merely a shared mathematical pattern is a
question DCR does not settle; we observe only that process ontology and
DCR converge on the same picture of reality as constituted by events of
constraint resolution rather than by persistent substances.

\subsection{Relation to Neural-Network Universe Proposals}

Vanchurin's ``world as a neural network'' program
\citep{vanchurin2020world} is the closest existing proposal to DCR in
ambition: it posits that the universe is fundamentally a learning
system and derives Schr\"odinger/Madelung-like and
Einstein--Hilbert-like effective forms from network update rules under
specific assumptions (see \cref{rem:nn-universe} for a summary).  The
key structural overlap is the two-tier dynamics---trainable variables
evolving on a slow timescale and hidden neuron states on a fast
timescale---which maps directly onto DCR's timescale separation
between inter-group and intra-group dynamics
(\cref{def:timescale}).  Vanchurin's ``second law of learning''
(entropy production from stochasticity vs.\ entropy destruction from
learning) is a special case of DCR's exploration--resolution balance:
stochastic updates inject variability; learning rules reduce
constraint violation; stable effective laws emerge at the balance
point.

The differences are instructive.  Vanchurin's proposal is
\emph{substrate-specific}: it commits to the universe being literally
a neural network, with particular thermodynamic and Onsager-symmetry
assumptions driving the recovery of known physics.  DCR is
\emph{substrate-agnostic}: it identifies the process
(explore--resolve--stabilize) without specifying what implements it.
A neural-network universe is one possible microphysics whose effective
behavior instantiates the DCR triad; other microphysics (discrete
ontic models \citep{powers2024statistical}, spin networks, causal
sets) could equally serve.  In this sense, DCR provides the
\emph{process-level} explanation of \emph{why} a neural-network
universe would produce stable macroscopic laws: because it implements
distributed constraint resolution, and DCR dynamics converge to
coherent attractors under generic conditions
(\cref{thm:convergence}).

\subsection{Implications for Artificial Intelligence}

Current AI systems (large language models, reinforcement learning agents)
implement the DCR triad in restricted form: stochastic sampling (exploration),
gradient descent or constraint propagation (resolution), convergence to
low-loss configurations (stabilization). DCR predicts that the ``intelligence''
of these systems is bounded by their cognitive depth: the number of nested
levels at which the explore--resolve--stabilize cycle operates simultaneously.
This suggests that advances in AI may come not from scaling individual layers
but from increasing organizational depth---more levels of nested constraint
resolution.

\subsection{Limitations and Open Problems}

\begin{enumerate}
  \item \textbf{Timescale separation.} The closure theorem
        (\cref{thm:closure}) requires timescale separation between intra-group
        and inter-group dynamics. While this condition holds in many physical
        systems (atomic vs.\ molecular, synaptic vs.\ network), proving closure
        under weaker conditions---overlapping timescales, continuous-time
        limits, or stochastic timescale ratios---remains open. The convergence
        rates derived in \cref{lem:macro-drift} depend on the separation
        parameter $\eta$; quantifying this dependence precisely for specific
        physical systems is an important next step.

  \item \textbf{Quantitative predictions.} While DCR predicts qualitative
        relationships (inverted-U, depth--adaptability, critical constraint
        density), deriving precise quantitative predictions requires
        specifying the constraint structure of particular physical systems,
        which is a substantial empirical program.

  \item \textbf{The goal problem.} DCR defines goals as attractors of the
        dynamics, which avoids teleology. But this means that any attractor
        counts as a ``goal,'' including pathological ones (e.g., a dead
        organism is a stable attractor of biochemical dynamics). A richer
        notion of goal---perhaps involving the \emph{maintenance} of
        exploration capacity, connecting to autopoiesis---may be needed.

  \item \textbf{Temporal structure and retrocausality.} The TIQM framing
        of quantum DCR (\cref{ssec:quantum}) involves advanced waves
        propagating backward in time, suggesting that constraint resolution
        can be atemporal---a relation among boundary conditions rather than
        a process with a definite temporal direction. The current DCR
        formalism (\cref{sec:framework}) is built on forward-time Markov
        chains, which cannot accommodate retrocausal constraint propagation.
        Extending the framework to atemporal or bidirectional constraint
        resolution---perhaps using the two-state vector formalism or
        path-integral methods---is needed to fully capture the quantum case
        and may reveal a deeper temporal structure underlying the DCR triad.

  \item \textbf{Consciousness.} DCR is a theory of cognition, not of
        consciousness. It explains the process by which systems explore,
        resolve, and stabilize, but does not address the ``hard problem''
        \citep{chalmers1995facing} of why any of this is accompanied by
        subjective experience. DCR is compatible with, but does not entail,
        the identity of cognition and consciousness.
\end{enumerate}

% ==========================================================================
\section{Conclusion}\label{sec:conclusion}
% ==========================================================================

We have presented the Distributed Constraint Resolution (DCR) framework, a
formal, scale-free characterization of cognition as the process by which
components explore degrees of freedom and converge through distributed
constraint resolution into coherent, goal-stabilizing patterns. We have shown
that:

\begin{enumerate}[nosep]
  \item The framework is mathematically precise, built on constraint networks,
        stochastic dynamics, and information-theoretic coherence
        (\cref{sec:framework}).
  \item Under timescale separation, it is closed under coarse-graining,
        providing a constructive account of composition addressing the combination problem
        and a natural measure
        of cognitive depth (\cref{sec:scale-free}).
  \item Fundamental physical processes satisfy the DCR axioms: as a
        detailed verification sketch for thermodynamic self-organization
        (\cref{ex:benard}), and structurally for quantum transactions,
        biological adaptation, and neural dynamics
        (\cref{sec:physics}). The quantum case, framed
        through the Transactional Interpretation, reveals that wavefunction
        collapse and natural selection are structurally identical instances
        of distributed constraint resolution. The discrete ontic model of
        \citet{powers2024statistical} provides independent evidence that
        quantum mechanics is compatible with---and may emerge from---the
        kind of finite combinatorial structure that DCR assumes
        (\cref{rem:micro-choices}); and \citet{vanchurin2020world}
        independently derives Schr\"odinger/Madelung-like and
        Einstein--Hilbert-like effective forms from a learning neural
        network, a substrate-specific instantiation of the DCR triad
        (\cref{rem:nn-universe}).
  \item The Free Energy Principle and Integrated Information Theory are
        recoverable as special cases (\cref{sec:recovery}).
  \item The framework makes falsifiable predictions about coherence--exploration
        tradeoffs, depth--adaptability relationships, and critical constraint
        densities (\cref{sec:predictions}).
\end{enumerate}

If DCR is correct, then cognition is not a biological accident but a
fundamental feature of physical reality---the process by which the universe
explores its own degrees of freedom and resolves into the coherent structures
we observe at every scale. The offer wave and the mutation, the handshake
and the selection, the transaction and the adapted species, the symbol
ordering and the quantum outcome: one process, many substrates.

\bigskip
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}

